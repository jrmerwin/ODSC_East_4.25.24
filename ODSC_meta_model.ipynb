{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83e5892-afde-4eaa-8d59-424ea6762ba9",
   "metadata": {},
   "source": [
    "## Base Model Swarm\n",
    "This is the second in a series of three notebooks for the ODSC presentation 'Harnessing GPT Assistants for Superior Model Ensembles: A Beginner's Guide to AI STacked-Classifiers' ODSC East -- Jason Merwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec730270-3fb1-4037-94be-d130ee73b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import io\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# define functions\n",
    "def delete_all_agents():\n",
    "    ''' Deletes all exising Assistants '''\n",
    "    # Fetch the list of assistants\n",
    "    my_assistants = client.beta.assistants.list(order=\"desc\", limit=20)\n",
    "    asst_ids = [asst.id for asst in my_assistants.data]\n",
    "    print(f'Deleting {len(asst_ids)} assistants.')\n",
    "    # Delete each assistant\n",
    "    for asst_id in asst_ids:\n",
    "        client.beta.assistants.delete(asst_id)\n",
    "        print(f\"Deleted assistant with ID: {asst_id}\")\n",
    "    print('Finished deleting all assistants')\n",
    "    \n",
    "def delete_all_assistant_files():\n",
    "    ''' Deletes all exising files uploaded to client using API key '''\n",
    "    # generate a files object\n",
    "    files_object = client.files.list()\n",
    "    # get a list comprehension\n",
    "    file_ids = [file.id for file in files_object.data]\n",
    "    print(f'Deleting {len(file_ids)} files.')\n",
    "    #delete them all\n",
    "    for file_id in file_ids:\n",
    "        client.files.delete(file_id)\n",
    "        print(f\"Deleted file with ID: {file_id}\")\n",
    "        time.sleep(1)\n",
    "    print('Finished deleting all files')   \n",
    "\n",
    "\n",
    "def upload_csv(file_name):\n",
    "    response = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"assistants\")\n",
    "    print(response)\n",
    "    file_id = response.id\n",
    "    return file_id\n",
    "\n",
    "def spin_up(target, base_instructions, file_id):\n",
    "    # create assistant\n",
    "    my_assistant = client.beta.assistants.create(\n",
    "        instructions=base_instructions,\n",
    "        name=\"agent\",\n",
    "        tools=[{\"type\": \"code_interpreter\"}],\n",
    "        model=\"gpt-4-turbo-preview\", #\"gpt-4-1106-preview\", # \"gpt-4\", # \"gpt-3.5-turbo-1106\", \"gpt-4-turbo-preview\"\n",
    "        file_ids=file_id)\n",
    "    message_string = \"Please execute your ACTIONS on the csv file, the target field is \" + target\n",
    "    # Create a Thread\n",
    "    thread = client.beta.threads.create()\n",
    "    # Add a Message to a Thread\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= message_string)\n",
    "    # Run the Assistant\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=my_assistant.id)\n",
    "    return my_assistant, thread, run \n",
    "    print('Finished creating Assistants')\n",
    "    #assistant, thread, run = spin_up(n, base_instructions, file_id)    \n",
    "    \n",
    "def catch_response(assistant, thread, run):\n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id)\n",
    "    print('########################')\n",
    "    print('Checking for response...')\n",
    "    # Handle None response\n",
    "    if run_status is None:\n",
    "        print(\"No response yet\")\n",
    "        return None, None  # Return a tuple of None values to match the expected return type\n",
    "    # Handle non-completed response\n",
    "    if run_status.status != 'completed':\n",
    "        print(\"Response status is not 'completed'\")\n",
    "        return None, None\n",
    "    # Handle completed response\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id)\n",
    "        contents = []  # Initialize an empty list to store contents\n",
    "        # Loop through messages and process content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "                contents.append(content)  # Append content to the list\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        return messages, contents  # Return messages and a list of contents\n",
    "    else:\n",
    "        print('Unable to retrieve message')\n",
    "        return None, None\n",
    "\n",
    "def create_dataframes_from_messages(messages, client):\n",
    "    loop_dfs = []\n",
    "    # Check if messages is None or messages.data is empty\n",
    "    if messages is None or not messages.data:\n",
    "        print(\"No messages data found.\")\n",
    "        return loop_dfs\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "    # Loop through each file ID and create a DataFrame\n",
    "    for file_id in message_ids:\n",
    "        # Read the file content\n",
    "        file_data = client.files.content(file_id)\n",
    "        # Check if file_data is None\n",
    "        if file_data is None:\n",
    "            print(f\"No content found for file_id: {file_id}\")\n",
    "            continue  # Skip this iteration and proceed with the next file_id\n",
    "        file_data_bytes = file_data.read()\n",
    "        file_like_object = io.BytesIO(file_data_bytes)\n",
    "        # Create a DataFrame from the file-like object and append\n",
    "        df = pd.read_csv(file_like_object)\n",
    "        loop_dfs.append(df)\n",
    "    return loop_dfs   \n",
    "\n",
    "def calculate_model_accuracies(df):\n",
    "    model_accuracy_dict = {}\n",
    "    # Filter columns that contain probability predictions\n",
    "    prediction_columns = [col for col in df.columns if \"_status\" in col]\n",
    "    \n",
    "    for col in prediction_columns:\n",
    "        # Assuming binary classification with 0.5 threshold\n",
    "        predicted_classes = df[col].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        actual_classes = df[f'{target}']\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "        # Extract model name from column name \n",
    "        model_name = col.split(\"_status\")[0]\n",
    "        model_accuracy_dict[model_name] = accuracy\n",
    "    \n",
    "    return model_accuracy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef63290-48df-46eb-bff6-1477c980f267",
   "metadata": {},
   "source": [
    "# Initialize API Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d31fc1-742a-4eaa-b2a8-bb4206640610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659c00-3167-48f1-9a94-c5d7f22ea536",
   "metadata": {},
   "source": [
    "# check training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065661d2-92bd-4bf4-94fc-ab868c390f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use the feature engineer output\n",
    "encoded_train = pd.read_csv('meta_train_df.csv')\n",
    "encoded_val = pd.read_csv('meta_val_df.csv')\n",
    "target = 'HeartDisease'\n",
    "\n",
    "display(encoded_train.head())\n",
    "display(encoded_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ef89a-6978-4105-82b5-18dcc82c5334",
   "metadata": {},
   "source": [
    "# Create the Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5a694-eb7f-481d-aab0-97ad411d0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make sure any existing bots and files are cleaned up\n",
    "delete_all_agents()   \n",
    "delete_all_assistant_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74626e1f-3bde-4ccf-b0e6-9b00a780d0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model types and file names for iteration\n",
    "model_types = ['Logistic_Regression', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'Random_Forest', 'Extra_Trees_Random_Forest', 'Support Vector Machine']\n",
    "\n",
    "train_id = upload_csv(f'meta_train_df.csv')\n",
    "val_id = upload_csv(f'meta_val_df.csv')\n",
    "file_ids = [train_id, val_id]\n",
    "\n",
    "agents = []\n",
    "\n",
    "for i in model_types:\n",
    "    print(f'Creating {i} assistant')\n",
    "    \n",
    "    #assign loop version of models and file names\n",
    "    model = i\n",
    "    print('provided these files')\n",
    "    print(file_ids)\n",
    "    instructions = instructions = f'''\n",
    "    You are a data scientist who will build and test a meta model with data output from several base models in the provided csv files.\n",
    "    When the user asks you to perform your ACTIONS, carry out the described ACTIONS on the provided files.\n",
    "    The target variable is '{target}'.\n",
    "    There is an id column to be maintained, unaltered and returned in the output called \"row_id\". It should be excluded when training the model.\n",
    "\n",
    "    ACTIONS:\n",
    "\n",
    "    1.The meta data has been prepared for training and testing a {model} classification model to predict the target variable '{target}'.\n",
    "    2.Train a {model} classification model using default hyper-parameter values and cross validation on the training data {train_id}. \n",
    "    3.Use the trained model to get the '{target}' prediction probabilities from the provided validation data in file {val_id}. It should produce a class probability scores.\n",
    "    4.Create Table 1 from the validation data class prediction probabilities. The table should have three columns: one for the target values in {val_id} called '{target}', one for the 'row_id' value, and a single column for the predicted status probability called '{model[:4]}_status'.\n",
    "    5.Prepare Table 1 as a CSV file for the user to download.   \n",
    "    6.Please do not respond until the table is ready for download.\n",
    "\n",
    "    DO NOT:\n",
    "    1. Do not return any images.\n",
    "    2. Do not return any other tables besides Table 1.\n",
    "    3. Do not include row_id as a feature in the training of the model.\n",
    "    4. Do not respond before the validation table is ready for download.\n",
    "    '''  \n",
    "\n",
    "    # spin up for each model type and store return object\n",
    "    assistant, thread, run = spin_up(f'{target}', instructions, file_ids) \n",
    "    agents.append((assistant, thread, run, model))  \n",
    "    print()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30fa14-0bda-4790-aebf-277b4b0e2816",
   "metadata": {},
   "source": [
    "# Catch the Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82616b43-e941-4ded-af4a-40628e6c16fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run a loop to catch the Agent responses\n",
    "time.sleep(360) \n",
    "\n",
    "agent_responses = []\n",
    "for assistant, thread, run, model, in agents:\n",
    "    messages, content = catch_response(assistant, thread, run) \n",
    "    agent_responses.append((messages, content, model, assistant))\n",
    "    time.sleep(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facb0a0-342f-42f5-a566-2e1fc0872374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract dataframes and compile\n",
    "df_list = []\n",
    "for messages, content, model, assistant in agent_responses:\n",
    "    dataframes = create_dataframes_from_messages(messages, client)\n",
    "    assistant_id = assistant.id\n",
    "    df_list.append([dataframes, model, assistant_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98c453-1d59-4155-a4fa-d283bd5e0362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c9e2b-47f0-42c7-b8ea-4f1be2e23e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary\n",
    "meta_results_df_dict = {}\n",
    "failures = []\n",
    "\n",
    "# Loop through and capture testing data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][0]) \n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        meta_results_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadc690-9123-4849-9358-a9c4cbfa9e7d",
   "metadata": {},
   "source": [
    "# Prepare Scored Validation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc1c86-f649-4e4b-8e2b-a71c8e3db14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_keys = list(meta_results_df_dict.keys())\n",
    "first_key = list_of_keys[0]\n",
    "meta_results_data = meta_results_df_dict[first_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in meta_results_df_dict:\n",
    "    if key != first_key and key not in failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = meta_results_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_results_data = meta_results_data.merge(cols_to_join, on=['row_id', f'{target}'], how='left')\n",
    "        print(f'joined to {key}')\n",
    "        \n",
    "display(meta_results_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d2418-f5ff-49f6-bc10-89796c0ea817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the meta training file\n",
    "meta_results_data.to_csv('meta_results_data.csv', index=False)\n",
    "meta_results_data_df = pd.read_csv('meta_results_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4da68-e9f6-4fef-805d-5764eeca9032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_dict = calculate_model_accuracies(meta_results_data_df)\n",
    "\n",
    "# save both rounds of performance outputs and join in third notebook for a graphic\n",
    "base_model_accuracy_df = pd.read_csv('base_model_accuracy.csv')\n",
    "\n",
    "meta_model_accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index').reset_index()\n",
    "meta_model_accuracy_df.columns = ['Model', 'Accuracy_meta']\n",
    "result = pd.merge(base_model_accuracy_df, meta_model_accuracy_df, left_on='Model', right_on='Model', how='left')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47a578-82c0-49af-8120-c4209826a3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = result.copy()\n",
    "\n",
    "# Setting the positions and width for the bars\n",
    "positions = list(range(len(df['Model'])))\n",
    "width = 0.35\n",
    "\n",
    "# Plotting accuracy1 and accuracy2 bars for each model type\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar([p - width/2 for p in positions], df['Accuracy_base'], width, alpha=0.5, color='b', label='Accuracy base')\n",
    "plt.bar([p + width/2 for p in positions], df['Accuracy_meta'], width, alpha=0.5, color='r', label='Accuracy meta')\n",
    "\n",
    "# Adding some labels and a title\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparison of Model Accuracies')\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(df['Model'])\n",
    "\n",
    "# Setting the y-axis to start at 0.5 and adjust upper limit appropriately\n",
    "ax.set_ylim([0.55, max(df['Accuracy_base'].max(), df['Accuracy_meta'].max()) + 0.05])  # Adjust upper limit if needed\n",
    "\n",
    "# Adding a legend to show which bars represent what\n",
    "plt.legend(['Accuracy base', 'Accuracy meta'], loc='lower left', framealpha=1)\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a3f11-6f70-402f-b84d-6a1689d69d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
