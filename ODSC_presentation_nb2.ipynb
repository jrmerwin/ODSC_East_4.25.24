{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83e5892-afde-4eaa-8d59-424ea6762ba9",
   "metadata": {},
   "source": [
    "## Base Model Swarm\n",
    "This is the second in a series of three notebooks for the ODSC presentation 'Harnessing GPT Assistants for Superior Model Ensembles: A Beginner's Guide to AI STacked-Classifiers' ODSC East -- Jason Merwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec730270-3fb1-4037-94be-d130ee73b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import io\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# define functions\n",
    "\n",
    "def delete_all_agents():\n",
    "    ''' Deletes all exising Assistants '''\n",
    "    # Fetch the list of assistants\n",
    "    my_assistants = client.beta.assistants.list(order=\"desc\", limit=20)\n",
    "    asst_ids = [asst.id for asst in my_assistants.data]\n",
    "    print(f'Deleting {len(asst_ids)} assistants.')\n",
    "    # Delete each assistant\n",
    "    for asst_id in asst_ids:\n",
    "        client.beta.assistants.delete(asst_id)\n",
    "        print(f\"Deleted assistant with ID: {asst_id}\")\n",
    "    print('Finished deleting all assistants')\n",
    "    \n",
    "def delete_all_assistant_files():\n",
    "    ''' Deletes all exising files uploaded to client using API key '''\n",
    "    # generate a files object\n",
    "    files_object = client.files.list()\n",
    "    # get a list comprehension\n",
    "    file_ids = [file.id for file in files_object.data]\n",
    "    print(f'Deleting {len(file_ids)} files.')\n",
    "    #delete them all\n",
    "    for file_id in file_ids:\n",
    "        client.files.delete(file_id)\n",
    "        print(f\"Deleted file with ID: {file_id}\")\n",
    "        time.sleep(1)\n",
    "    print('Finished deleting all files')   \n",
    "\n",
    "def upload_csv(file_name):\n",
    "    response = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"assistants\")\n",
    "    print(response)\n",
    "    file_id = response.id\n",
    "    return file_id\n",
    "\n",
    "def spin_up(target, base_instructions, file_id):\n",
    "    # create assistant\n",
    "    my_assistant = client.beta.assistants.create(\n",
    "        instructions=base_instructions,\n",
    "        name=\"agent\",\n",
    "        tools=[{\"type\": \"code_interpreter\"}],\n",
    "        model=\"gpt-4-turbo-preview\", #\"gpt-4-1106-preview\", # \"gpt-4\", # \"gpt-3.5-turbo-1106\", \"gpt-4-turbo-preview\"\n",
    "        file_ids=file_id)\n",
    "    message_string = \"Please execute your ACTIONS on the csv file, the target field is \" + target\n",
    "    # Create a Thread\n",
    "    thread = client.beta.threads.create()\n",
    "    # Add a Message to a Thread\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= message_string)\n",
    "    # Run the Assistant\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=my_assistant.id)\n",
    "    return my_assistant, thread, run \n",
    "    print('Finished creating Assistants')\n",
    "    \n",
    "def catch_response(assistant, thread, run):\n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id)\n",
    "    print('########################')\n",
    "    print('Checking for response...')\n",
    "    # Handle None response\n",
    "    if run_status is None:\n",
    "        print(\"No response yet\")\n",
    "        return None, None  # Return a tuple of None values to match the expected return type\n",
    "    # Handle non-completed response\n",
    "    if run_status.status != 'completed':\n",
    "        print(\"Response status is not 'completed'\")\n",
    "        return None, None\n",
    "    # Handle completed response\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id)\n",
    "        contents = []  # Initialize an empty list to store contents\n",
    "        # Loop through messages and process content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "                contents.append(content)  # Append content to the list\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        return messages, contents  # Return messages and a list of contents\n",
    "    else:\n",
    "        print('Unable to retrieve message')\n",
    "        return None, None\n",
    "\n",
    "def create_dataframes_from_messages(messages, client):\n",
    "    loop_dfs = []\n",
    "\n",
    "    # Check if messages is None or messages.data is empty\n",
    "    if messages is None or not messages.data:\n",
    "        print(\"No messages data found.\")\n",
    "        return loop_dfs\n",
    "\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "\n",
    "    # Loop through each file ID and create a DataFrame\n",
    "    for file_id in message_ids:\n",
    "        # Read the file content\n",
    "        file_data = client.files.content(file_id)\n",
    "\n",
    "        # Check if file_data is None\n",
    "        if file_data is None:\n",
    "            print(f\"No content found for file_id: {file_id}\")\n",
    "            continue  # Skip this iteration and proceed with the next file_id\n",
    "\n",
    "        file_data_bytes = file_data.read()\n",
    "        file_like_object = io.BytesIO(file_data_bytes)\n",
    "\n",
    "        # Create a DataFrame from the file-like object and append\n",
    "        df = pd.read_csv(file_like_object)\n",
    "        loop_dfs.append(df)\n",
    "\n",
    "    return loop_dfs   \n",
    "\n",
    "def calculate_model_accuracies(df):\n",
    "    model_accuracy_dict = {}\n",
    "    # Filter columns that contain probability predictions\n",
    "    prediction_columns = [col for col in df.columns if \"_prob\" in col]\n",
    "    \n",
    "    for col in prediction_columns:\n",
    "        # Assuming binary classification with 0.5 threshold\n",
    "        predicted_classes = df[col].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        actual_classes = df[f'{target}']\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "        # Extract model name from column name \n",
    "        model_name = col.split(\"_prob\")[0]\n",
    "        model_accuracy_dict[model_name] = accuracy\n",
    "    \n",
    "    return model_accuracy_dict\n",
    "\n",
    "def calculate_model_metrics(df, target):\n",
    "    model_metrics_dict = {}\n",
    "    # Filter columns that contain probability predictions\n",
    "    prediction_columns = [col for col in df.columns if \"_prob\" in col]\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        # Assuming binary classification with 0.5 threshold\n",
    "        predicted_classes = df[col].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        actual_classes = df[target]\n",
    "        # Calculate accuracy, precision, recall, and F1-score\n",
    "        accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "        precision = precision_score(actual_classes, predicted_classes, zero_division=0)\n",
    "        recall = recall_score(actual_classes, predicted_classes, zero_division=0)\n",
    "        f1 = f1_score(actual_classes, predicted_classes, zero_division=0)\n",
    "        # Extract model name from column name \n",
    "        model_name = col.split(\"_prob\")[0]\n",
    "        # Store the metrics in a dictionary with the model name as key\n",
    "        model_metrics_dict[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "\n",
    "    return model_metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef63290-48df-46eb-bff6-1477c980f267",
   "metadata": {},
   "source": [
    "# Initialize API Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d31fc1-742a-4eaa-b2a8-bb4206640610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659c00-3167-48f1-9a94-c5d7f22ea536",
   "metadata": {},
   "source": [
    "# check training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065661d2-92bd-4bf4-94fc-ab868c390f90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Crunchiness</th>\n",
       "      <th>Juiciness</th>\n",
       "      <th>Ripeness</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Sweetness*Ripeness</th>\n",
       "      <th>Size*Ripeness</th>\n",
       "      <th>Size*Acidity</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-3.970049</td>\n",
       "      <td>-2.512336</td>\n",
       "      <td>5.346330</td>\n",
       "      <td>-1.012009</td>\n",
       "      <td>1.844900</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>-0.491590</td>\n",
       "      <td>1.763432</td>\n",
       "      <td>-1.309480</td>\n",
       "      <td>1.951638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.195217</td>\n",
       "      <td>-2.839257</td>\n",
       "      <td>3.664059</td>\n",
       "      <td>1.588232</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.867530</td>\n",
       "      <td>-0.722809</td>\n",
       "      <td>3.178681</td>\n",
       "      <td>-1.036887</td>\n",
       "      <td>0.863914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.292024</td>\n",
       "      <td>-1.351282</td>\n",
       "      <td>-1.738429</td>\n",
       "      <td>-0.342616</td>\n",
       "      <td>2.838636</td>\n",
       "      <td>-0.038033</td>\n",
       "      <td>2.621636</td>\n",
       "      <td>0.066118</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>-0.765580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.657196</td>\n",
       "      <td>-2.271627</td>\n",
       "      <td>1.324874</td>\n",
       "      <td>-0.097875</td>\n",
       "      <td>3.637970</td>\n",
       "      <td>-3.413761</td>\n",
       "      <td>0.790723</td>\n",
       "      <td>-4.522803</td>\n",
       "      <td>2.243510</td>\n",
       "      <td>-0.519660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.364217</td>\n",
       "      <td>-1.296612</td>\n",
       "      <td>-0.384658</td>\n",
       "      <td>-0.553006</td>\n",
       "      <td>3.030874</td>\n",
       "      <td>-1.303849</td>\n",
       "      <td>0.501984</td>\n",
       "      <td>0.501536</td>\n",
       "      <td>-1.778733</td>\n",
       "      <td>0.684815</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3995</td>\n",
       "      <td>0.059386</td>\n",
       "      <td>-1.067408</td>\n",
       "      <td>-3.714549</td>\n",
       "      <td>0.473052</td>\n",
       "      <td>1.697986</td>\n",
       "      <td>2.244055</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>-8.335650</td>\n",
       "      <td>0.133266</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3996</td>\n",
       "      <td>-0.293118</td>\n",
       "      <td>1.949253</td>\n",
       "      <td>-0.204020</td>\n",
       "      <td>-0.640196</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>-1.087900</td>\n",
       "      <td>1.854235</td>\n",
       "      <td>0.221953</td>\n",
       "      <td>0.318883</td>\n",
       "      <td>-0.543510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3997</td>\n",
       "      <td>-2.634515</td>\n",
       "      <td>-2.138247</td>\n",
       "      <td>-2.440461</td>\n",
       "      <td>0.657223</td>\n",
       "      <td>2.199709</td>\n",
       "      <td>4.763859</td>\n",
       "      <td>-1.334611</td>\n",
       "      <td>-11.626014</td>\n",
       "      <td>-12.550460</td>\n",
       "      <td>3.516054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3998</td>\n",
       "      <td>-4.008004</td>\n",
       "      <td>-1.779337</td>\n",
       "      <td>2.366397</td>\n",
       "      <td>-0.200329</td>\n",
       "      <td>2.161435</td>\n",
       "      <td>0.214488</td>\n",
       "      <td>-2.229720</td>\n",
       "      <td>0.507565</td>\n",
       "      <td>-0.859670</td>\n",
       "      <td>8.936725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>3999</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>-1.715505</td>\n",
       "      <td>0.121217</td>\n",
       "      <td>-1.154075</td>\n",
       "      <td>1.266677</td>\n",
       "      <td>-0.776571</td>\n",
       "      <td>1.599796</td>\n",
       "      <td>-0.094134</td>\n",
       "      <td>-0.216306</td>\n",
       "      <td>0.445607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id      Size    Weight  Sweetness  Crunchiness  Juiciness  Ripeness  \\\n",
       "0          0 -3.970049 -2.512336   5.346330    -1.012009   1.844900  0.329840   \n",
       "1          1 -1.195217 -2.839257   3.664059     1.588232   0.853286  0.867530   \n",
       "2          2 -0.292024 -1.351282  -1.738429    -0.342616   2.838636 -0.038033   \n",
       "3          3 -0.657196 -2.271627   1.324874    -0.097875   3.637970 -3.413761   \n",
       "4          4  1.364217 -1.296612  -0.384658    -0.553006   3.030874 -1.303849   \n",
       "...      ...       ...       ...        ...          ...        ...       ...   \n",
       "3995    3995  0.059386 -1.067408  -3.714549     0.473052   1.697986  2.244055   \n",
       "3996    3996 -0.293118  1.949253  -0.204020    -0.640196   0.024523 -1.087900   \n",
       "3997    3997 -2.634515 -2.138247  -2.440461     0.657223   2.199709  4.763859   \n",
       "3998    3998 -4.008004 -1.779337   2.366397    -0.200329   2.161435  0.214488   \n",
       "3999    3999  0.278540 -1.715505   0.121217    -1.154075   1.266677 -0.776571   \n",
       "\n",
       "       Acidity  Sweetness*Ripeness  Size*Ripeness  Size*Acidity  Quality  \n",
       "0    -0.491590            1.763432      -1.309480      1.951638        1  \n",
       "1    -0.722809            3.178681      -1.036887      0.863914        1  \n",
       "2     2.621636            0.066118       0.011107     -0.765580        0  \n",
       "3     0.790723           -4.522803       2.243510     -0.519660        1  \n",
       "4     0.501984            0.501536      -1.778733      0.684815        1  \n",
       "...        ...                 ...            ...           ...      ...  \n",
       "3995  0.137784           -8.335650       0.133266      0.008183        0  \n",
       "3996  1.854235            0.221953       0.318883     -0.543510        1  \n",
       "3997 -1.334611          -11.626014     -12.550460      3.516054        0  \n",
       "3998 -2.229720            0.507565      -0.859670      8.936725        1  \n",
       "3999  1.599796           -0.094134      -0.216306      0.445607        1  \n",
       "\n",
       "[4000 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the feature engineer output\n",
    "target = 'Quality'\n",
    "encoded_train = pd.read_csv('feature_engineer_output_1b.csv')\n",
    "\n",
    "#optional: use the original dataset instead\n",
    "#encoded_train = pd.read_csv('pre_assistant_train.csv')\n",
    "\n",
    "#add a row id \n",
    "encoded_train = encoded_train.reset_index()\n",
    "encoded_train = encoded_train.rename(columns={'index': 'row_id'})\n",
    "encoded_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ef89a-6978-4105-82b5-18dcc82c5334",
   "metadata": {},
   "source": [
    "# Create the Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e5a694-eb7f-481d-aab0-97ad411d0727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 1 assistants.\n",
      "Deleted assistant with ID: asst_WXbgpKUFktsfMO84lXhz6NIQ\n",
      "Finished deleting all assistants\n",
      "Deleting 2 files.\n",
      "Deleted file with ID: file-8grttuOZwzwJj3Vz94864D0A\n",
      "Deleted file with ID: file-dpRMdyvBAbZU0So974wbXYAM\n",
      "Finished deleting all files\n"
     ]
    }
   ],
   "source": [
    "# first make sure any existing bots and files are cleaned up\n",
    "delete_all_agents()   \n",
    "delete_all_assistant_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcda3ab-fdfb-406f-853d-42bb9a4fe467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reserve 20% of training data to be used as \"inference\" data\n",
    "train_set, val_set = train_test_split(encoded_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#save the files\n",
    "train_set.to_csv('encoded_train.csv', index=False)\n",
    "val_set.to_csv('encoded_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463c3a52-3e56-4fe9-9496-6a56368806b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-SZHbZR6fxxdD5Obv88BdcbGV', bytes=485371, created_at=1714062846, filename='encoded_train.csv', object='file', purpose='assistants', status='processed', status_details=None)\n",
      "FileObject(id='file-D6VAYUYWFjY4unDFRTWvoXde', bytes=121451, created_at=1714062847, filename='encoded_val.csv', object='file', purpose='assistants', status='processed', status_details=None)\n",
      "Creating Logistic_Regression assistant\n",
      "provided these files\n",
      "['file-SZHbZR6fxxdD5Obv88BdcbGV', 'file-D6VAYUYWFjY4unDFRTWvoXde']\n",
      "\n",
      "Creating DecisionTreeClassifier assistant\n",
      "provided these files\n",
      "['file-SZHbZR6fxxdD5Obv88BdcbGV', 'file-D6VAYUYWFjY4unDFRTWvoXde']\n",
      "\n",
      "Creating KNeighborsClassifier assistant\n",
      "provided these files\n",
      "['file-SZHbZR6fxxdD5Obv88BdcbGV', 'file-D6VAYUYWFjY4unDFRTWvoXde']\n",
      "\n",
      "Creating Random_Forest assistant\n",
      "provided these files\n",
      "['file-SZHbZR6fxxdD5Obv88BdcbGV', 'file-D6VAYUYWFjY4unDFRTWvoXde']\n",
      "\n",
      "Creating Extra_Trees_Random_Forest assistant\n",
      "provided these files\n",
      "['file-SZHbZR6fxxdD5Obv88BdcbGV', 'file-D6VAYUYWFjY4unDFRTWvoXde']\n",
      "\n",
      "Creating Support Vector Machine assistant\n",
      "provided these files\n",
      "['file-SZHbZR6fxxdD5Obv88BdcbGV', 'file-D6VAYUYWFjY4unDFRTWvoXde']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the model types here by description\n",
    "model_types = ['Logistic_Regression', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'Random_Forest', 'Extra_Trees_Random_Forest', 'Support Vector Machine']\n",
    "\n",
    "train_id = upload_csv(f'encoded_train.csv')\n",
    "val_id = upload_csv(f'encoded_val.csv')\n",
    "file_ids = [train_id, val_id]\n",
    "\n",
    "agents = []\n",
    "\n",
    "for i in model_types:\n",
    "    print(f'Creating {i} assistant')\n",
    "    \n",
    "    #assign loop version of models and file names\n",
    "    model = i\n",
    "    print('provided these files')\n",
    "    print(file_ids)\n",
    "    instructions = instructions = f'''\n",
    "    You are a data scientist who will build and test a predictive model with data from the provided csv file.\n",
    "    This model will be base model for a stacked model ensemble, thus the predictions on the training data will be used as input for a meta model. \n",
    "    When the user asks you to perform your ACTIONS, carry out the described ACTIONS on the provided files.\n",
    "    The target variable is '{target}'.\n",
    "    There is an id column to be maintained, unaltered and returned in the output called \"row_id\". This column should be excluded when training the model.\n",
    "\n",
    "    ACTIONS:\n",
    "\n",
    "    1.The data has been prepared for training a {model} classification model to predict the target variable '{target}'.\n",
    "    2.Split the training data in the file {train_id} into 5 K-folds for cross-validation. Each fold should serve once as a validation set while the remaining folds serve as training sets.\n",
    "    3.Train a {model} classification model using default hyper-parameter values on each training set derived from the K folds, ensuring the target variable is '{target}'.\n",
    "    4.For each fold, use the trained {model} to predict the '{target}' on its corresponding validation set. Ensure the predictions are probabilities.\n",
    "    5.Compile the out-of-fold predictions into a single dataset. This dataset should include the 'row_id' from the testing set and the predicted probabilities. Name the columns as follows: 'row_id' and '{model[:4]}_prob'.\n",
    "    6.Save this compiled dataset as a CSV file named 'out_of_fold_predictions.csv' and prepare it for the user to download. This file will be used for training the meta-model.\n",
    "    7.Now use the trained models to score the validation data in the file {val_id} containing the same target column '{target}'. Average their scores for each row in the validation data and compile the results in the same way as before and prepare it for the user to download as a CSV file names 'valiation_predicitons.csv'\n",
    "    8.Both tables should contain 2 columns: row_id and '{model[:4]}_prob'.\n",
    "    9.Please only respond once, with both tables once they are ready for download.\n",
    "    \n",
    "    DO NOT:\n",
    "    1. Do not return any images.\n",
    "    2. Do not return any other tables besides the tables 'out_of_fold_predictions.csv' and 'valiation_predicitons.csv'\n",
    "    3. Do not include row_id as a feature in the training of the model.\n",
    "    4. Do not respond before both tables are ready for download.\n",
    "\n",
    "    '''  \n",
    "\n",
    "    # spin up for each model type and store return object\n",
    "    assistant, thread, run = spin_up(f'{target}', instructions, file_ids) \n",
    "    agents.append((assistant, thread, run, model))  \n",
    "    print()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30fa14-0bda-4790-aebf-277b4b0e2816",
   "metadata": {},
   "source": [
    "# Catch the Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82616b43-e941-4ded-af4a-40628e6c16fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "Checking for response...\n",
      "Assistant: Both tables are ready for download:\n",
      "\n",
      "- For the out-of-fold predictions, download [out_of_fold_predictions.csv](sandbox:/mnt/data/out_of_fold_predictions.csv).\n",
      "- For the validation predictions, download [validation_predictions.csv](sandbox:/mnt/data/validation_predictions.csv).\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: I have completed the requested actions and generated the two CSV files:\n",
      "\n",
      "- The out-of-fold predictions from the training data: [Download out_of_fold_predictions.csv](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- The averaged scores for each row in the validation data: [Download validation_predictions.csv](sandbox:/mnt/data/validation_predictions.csv)\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: I have completed the tasks and the files are ready for download:\n",
      "\n",
      "- **Out-of-fold predictions**: [Download out_of_fold_predictions.csv](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- **Validation predictions**: [Download validation_predictions.csv](sandbox:/mnt/data/validation_predictions.csv)\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: Both tables are ready for download:\n",
      "\n",
      "- [Download out-of-fold predictions here](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- [Download validation predictions here](sandbox:/mnt/data/validation_predictions.csv)\n",
      "Assistant: The predictions for the unseen validation data, with 'row_id' and the averaged predicted probabilities ('Rand_prob'), are now prepared. Following this, I'm saving the compiled out-of-fold predictions and the compiled predictions for the unseen validation data as CSV files. These files, named 'out_of_fold_predictions.csv' and 'validation_predictions.csv' respectively, will be ready for download shortly.\n",
      "Assistant: The compilation of out-of-fold predictions with 'row_id' and the predicted probabilities ('Rand_prob') for the training data is ready. Next, I'll predict the 'Quality' on the unseen validation data provided in the second file using the trained Random Forest models and average their scores for each row in the validation data. After that, I will save both datasets as CSV files and provide them for download. Let's proceed with predicting and compiling scores on the unseen validation data.\n",
      "Assistant: I will start by executing the described ACTIONS on the provided CSV file:\n",
      "\n",
      "1. Load the given CSV file for training.\n",
      "2. Split the data into 5 K-folds for cross-validation.\n",
      "3. Train a Random_Forest classification model using each training set.\n",
      "4. Make predictions on the validation set in each fold, ensuring that the predictions are probabilities.\n",
      "5. Compile the predictions along with 'row_id' into a single dataset and save it as 'out_of_fold_predictions.csv'.\n",
      "6. Use the trained models to score the unseen validation data provided in the second file.\n",
      "7. Compile their scores averaged for each row with their 'row_id' into another CSV file named 'validation_predictions.csv'.\n",
      "8. Provide both CSV files for download once ready. \n",
      "\n",
      "Let's begin with step 1.\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: Both tables have been successfully compiled and saved. You can download them using the following links:\n",
      "\n",
      "- [Out of Fold Predictions](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- [Validation Predictions](sandbox:/mnt/data/valiation_predicitons.csv)\n",
      "Assistant: I will begin with the following steps:\n",
      "\n",
      "1. Load the training data from the provided file.\n",
      "2. Apply 5-fold cross-validation to split the data.\n",
      "3. For each fold, train a Extra Trees Random Forest classification model using default hyper-parameters.\n",
      "4. Generate out-of-fold predictions for 'Quality' as probabilities.\n",
      "5. Compile these predictions into a single dataset as requested.\n",
      "6. Repeat the process for the validation data using the trained models to generate and compile predictions.\n",
      "7. Save both sets of compiled predictions into respective CSV files as instructed.\n",
      "\n",
      "Let's start with the first part.\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: The actions have been executed and the results are compiled into two CSV files:\n",
      "\n",
      "1. **Out-of-Fold Predictions**: This file can be downloaded from [here](sandbox:/mnt/data/out_of_fold_predictions.csv).\n",
      "\n",
      "2. **Validation Predictions**: This file can be downloaded from [here](sandbox:/mnt/data/validation_predictions.csv).\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n"
     ]
    }
   ],
   "source": [
    "# run a loop to catch the Agent responses\n",
    "time.sleep(300) \n",
    "\n",
    "agent_responses = []\n",
    "for assistant, thread, run, model, in agents:\n",
    "    messages, content = catch_response(assistant, thread, run) \n",
    "    agent_responses.append((messages, content, model, assistant))\n",
    "    time.sleep(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4facb0a0-342f-42f5-a566-2e1fc0872374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract dataframes and compile\n",
    "df_list = []\n",
    "for messages, content, model, assistant in agent_responses:\n",
    "    dataframes = create_dataframes_from_messages(messages, client)\n",
    "    assistant_id = assistant.id\n",
    "    df_list.append([dataframes, model, assistant_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f6c9e2b-47f0-42c7-b8ea-4f1be2e23e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     row_id  Logi_prob\n",
      "0       555   0.641120\n",
      "1      3491   0.666901\n",
      "2       527   0.603839\n",
      "3      3925   0.524097\n",
      "4      2989   0.015525\n",
      "..      ...        ...\n",
      "795    1922   0.358516\n",
      "796     865   0.884039\n",
      "797    3943   0.934317\n",
      "798    1642   0.662319\n",
      "799    2483   0.862121\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Deci_prob\n",
      "0       555        1.0\n",
      "1      3491        0.0\n",
      "2       527        1.0\n",
      "3      3925        1.0\n",
      "4      2989        0.0\n",
      "..      ...        ...\n",
      "795    1922        0.0\n",
      "796     865        1.0\n",
      "797    3943        1.0\n",
      "798    1642        1.0\n",
      "799    2483        1.0\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  KNei_prob\n",
      "0       555       0.72\n",
      "1      3491       0.76\n",
      "2       527       0.68\n",
      "3      3925       1.00\n",
      "4      2989       0.00\n",
      "..      ...        ...\n",
      "795    1922       0.00\n",
      "796     865       1.00\n",
      "797    3943       1.00\n",
      "798    1642       0.76\n",
      "799    2483       0.96\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Rand_prob\n",
      "0       555      0.686\n",
      "1      3491      0.862\n",
      "2       527      0.570\n",
      "3      3925      0.732\n",
      "4      2989      0.022\n",
      "..      ...        ...\n",
      "795    1922      0.266\n",
      "796     865      0.982\n",
      "797    3943      0.976\n",
      "798    1642      0.772\n",
      "799    2483      0.718\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Extr_prob\n",
      "0       555      0.644\n",
      "1      3491      0.714\n",
      "2       527      0.528\n",
      "3      3925      0.788\n",
      "4      2989      0.040\n",
      "..      ...        ...\n",
      "795    1922      0.272\n",
      "796     865      0.960\n",
      "797    3943      0.960\n",
      "798    1642      0.728\n",
      "799    2483      0.700\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Supp_prob\n",
      "0       555   0.829235\n",
      "1      3491   0.834491\n",
      "2       527   0.378980\n",
      "3      3925   0.936887\n",
      "4      2989   0.009707\n",
      "..      ...        ...\n",
      "795    1922   0.278218\n",
      "796     865   0.974413\n",
      "797    3943   0.998720\n",
      "798    1642   0.868540\n",
      "799    2483   0.950816\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "assistants which failed to return a scored training data dataframe:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Capture the validation data scores\n",
    "val_data_df_dict = {}\n",
    "val_failures = []\n",
    "\n",
    "# Loop through and capture validation data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][0]) \n",
    "        print(df1)\n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        val_data_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        val_failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(val_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45598af6-ca07-41e4-b843-826db1ff4336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id  Logi_prob\n",
      "0          0   0.807077\n",
      "1          1   0.888458\n",
      "2          2   0.509400\n",
      "3          3   0.707898\n",
      "4          4   0.829310\n",
      "...      ...        ...\n",
      "3195    3994   0.907927\n",
      "3196    3995   0.059556\n",
      "3197    3996   0.587951\n",
      "3198    3998   0.885267\n",
      "3199    3999   0.639848\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Deci_prob\n",
      "0       3994          0\n",
      "1        423          1\n",
      "2       2991          0\n",
      "3       1221          0\n",
      "4        506          1\n",
      "...      ...        ...\n",
      "3195    1130          1\n",
      "3196    1294          0\n",
      "3197     860          1\n",
      "3198    3507          1\n",
      "3199    3174          1\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  KNei_prob\n",
      "0       3994        1.0\n",
      "1       1601        1.0\n",
      "2       1779        1.0\n",
      "3       3323        1.0\n",
      "4       2059        0.6\n",
      "...      ...        ...\n",
      "3195    1685        0.4\n",
      "3196    2135        0.6\n",
      "3197    1482        0.0\n",
      "3198    2169        0.4\n",
      "3199    3174        1.0\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Rand_prob\n",
      "0       3994       0.84\n",
      "1       1601       0.98\n",
      "2       1779       0.96\n",
      "3       3323       0.80\n",
      "4       2059       0.65\n",
      "...      ...        ...\n",
      "3195    1685       0.66\n",
      "3196    2135       0.53\n",
      "3197    1482       0.46\n",
      "3198    2169       0.42\n",
      "3199    3174       0.91\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Extr_prob\n",
      "0       3994       0.89\n",
      "1        423       0.58\n",
      "2       2991       0.00\n",
      "3       1221       0.69\n",
      "4        506       0.52\n",
      "...      ...        ...\n",
      "3195    1130       0.68\n",
      "3196    1294       0.11\n",
      "3197     860       0.97\n",
      "3198    3507       0.70\n",
      "3199    3174       0.78\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Supp_prob\n",
      "0       3994   0.956977\n",
      "1       1601   0.999990\n",
      "2       1779   0.989203\n",
      "3       3323   0.927012\n",
      "4       2059   0.791790\n",
      "...      ...        ...\n",
      "3195    1685   0.331663\n",
      "3196    2135   0.540341\n",
      "3197    1482   0.682491\n",
      "3198    2169   0.378919\n",
      "3199    3174   0.972780\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "assistants which failed to return a scored training data dataframe:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Capture the meta training data\n",
    "test_data_df_dict = {}\n",
    "test_failures = []\n",
    "\n",
    "# Loop through and capture testing data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][1]) \n",
    "        print(df1)\n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        test_data_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        test_failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(test_failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadc690-9123-4849-9358-a9c4cbfa9e7d",
   "metadata": {},
   "source": [
    "# Prepare Scored Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697748d1-16ab-4292-9082-b5eea9efb87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined to DecisionTreeClassifier\n",
      "joined to KNeighborsClassifier\n",
      "joined to Random_Forest\n",
      "joined to Extra_Trees_Random_Forest\n",
      "joined to Support Vector Machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Logi_prob</th>\n",
       "      <th>Deci_prob</th>\n",
       "      <th>KNei_prob</th>\n",
       "      <th>Rand_prob</th>\n",
       "      <th>Extr_prob</th>\n",
       "      <th>Supp_prob</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>555</td>\n",
       "      <td>0.641120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.829235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3491</td>\n",
       "      <td>0.666901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.834491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>527</td>\n",
       "      <td>0.603839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.378980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3925</td>\n",
       "      <td>0.524097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.936887</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2989</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.009707</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>1922</td>\n",
       "      <td>0.358516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.278218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>865</td>\n",
       "      <td>0.884039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.974413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>3943</td>\n",
       "      <td>0.934317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.998720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>1642</td>\n",
       "      <td>0.662319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.868540</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>2483</td>\n",
       "      <td>0.862121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.950816</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  Logi_prob  Deci_prob  KNei_prob  Rand_prob  Extr_prob  Supp_prob  \\\n",
       "0       555   0.641120        1.0       0.72      0.686      0.644   0.829235   \n",
       "1      3491   0.666901        0.0       0.76      0.862      0.714   0.834491   \n",
       "2       527   0.603839        1.0       0.68      0.570      0.528   0.378980   \n",
       "3      3925   0.524097        1.0       1.00      0.732      0.788   0.936887   \n",
       "4      2989   0.015525        0.0       0.00      0.022      0.040   0.009707   \n",
       "..      ...        ...        ...        ...        ...        ...        ...   \n",
       "795    1922   0.358516        0.0       0.00      0.266      0.272   0.278218   \n",
       "796     865   0.884039        1.0       1.00      0.982      0.960   0.974413   \n",
       "797    3943   0.934317        1.0       1.00      0.976      0.960   0.998720   \n",
       "798    1642   0.662319        1.0       0.76      0.772      0.728   0.868540   \n",
       "799    2483   0.862121        1.0       0.96      0.718      0.700   0.950816   \n",
       "\n",
       "     Quality  \n",
       "0          1  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  \n",
       "..       ...  \n",
       "795        0  \n",
       "796        1  \n",
       "797        1  \n",
       "798        1  \n",
       "799        1  \n",
       "\n",
       "[800 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_val_keys = list(val_data_df_dict.keys())\n",
    "first_val_key = list_of_val_keys[0]\n",
    "meta_val_data = val_data_df_dict[first_val_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in val_data_df_dict:\n",
    "    if key != first_val_key and key not in val_failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = val_data_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_val_data = meta_val_data.merge(cols_to_join, on='row_id', how='left')\n",
    "        print(f'joined to {key}')\n",
    "\n",
    "# add back label\n",
    "val_label_df = encoded_train[['row_id', 'Quality']]\n",
    "meta_val_data = meta_val_data.merge(val_label_df, on='row_id', how='left')\n",
    "\n",
    "display(meta_val_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6acc1c86-f649-4e4b-8e2b-a71c8e3db14c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined to DecisionTreeClassifier\n",
      "joined to KNeighborsClassifier\n",
      "joined to Random_Forest\n",
      "joined to Extra_Trees_Random_Forest\n",
      "joined to Support Vector Machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Logi_prob</th>\n",
       "      <th>Deci_prob</th>\n",
       "      <th>KNei_prob</th>\n",
       "      <th>Rand_prob</th>\n",
       "      <th>Extr_prob</th>\n",
       "      <th>Supp_prob</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.807077</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.976517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.888458</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.977460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.125335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.707898</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.963616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.829310</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.898430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>3994</td>\n",
       "      <td>0.907927</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.956977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>3995</td>\n",
       "      <td>0.059556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>3996</td>\n",
       "      <td>0.587951</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.717308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>3998</td>\n",
       "      <td>0.885267</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.880914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>3999</td>\n",
       "      <td>0.639848</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.855832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id  Logi_prob  Deci_prob  KNei_prob  Rand_prob  Extr_prob  \\\n",
       "0          0   0.807077          1        1.0       0.81       0.82   \n",
       "1          1   0.888458          1        1.0       0.97       0.97   \n",
       "2          2   0.509400          0        0.0       0.28       0.28   \n",
       "3          3   0.707898          1        1.0       0.90       0.93   \n",
       "4          4   0.829310          0        1.0       0.84       0.81   \n",
       "...      ...        ...        ...        ...        ...        ...   \n",
       "3195    3994   0.907927          0        1.0       0.84       0.89   \n",
       "3196    3995   0.059556          0        0.0       0.00       0.00   \n",
       "3197    3996   0.587951          1        0.8       0.70       0.78   \n",
       "3198    3998   0.885267          1        1.0       0.88       0.83   \n",
       "3199    3999   0.639848          1        0.8       0.82       0.80   \n",
       "\n",
       "      Supp_prob  Quality  \n",
       "0      0.976517        1  \n",
       "1      0.977460        1  \n",
       "2      0.125335        0  \n",
       "3      0.963616        1  \n",
       "4      0.898430        1  \n",
       "...         ...      ...  \n",
       "3195   0.956977        1  \n",
       "3196   0.014631        0  \n",
       "3197   0.717308        1  \n",
       "3198   0.880914        1  \n",
       "3199   0.855832        1  \n",
       "\n",
       "[3200 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_keys = list(test_data_df_dict.keys())\n",
    "first_key = list_of_keys[0]\n",
    "meta_training_data = test_data_df_dict[first_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in test_data_df_dict:\n",
    "    if key != first_key and key not in test_failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = test_data_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_training_data = meta_training_data.merge(cols_to_join, on='row_id', how='left')\n",
    "        print(f'joined to {key}')\n",
    "\n",
    "# add back label\n",
    "label_df = encoded_train[['row_id', 'Quality']]\n",
    "meta_training_data = meta_training_data.merge(label_df, on='row_id', how='left')\n",
    "\n",
    "display(meta_training_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388817c5-073c-436b-a47b-c19ed46cb0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up join (if necessary)\n",
    "# meta_training_data = meta_training_data.dropna()\n",
    "# display(meta_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300d2418-f5ff-49f6-bc10-89796c0ea817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the meta training file\n",
    "meta_training_data.to_csv('meta_train_df.csv', index=False)\n",
    "meta_train_df = pd.read_csv('meta_train_df.csv')\n",
    "\n",
    "# save the meta validation file (acting as inference data)\n",
    "meta_val_data.to_csv('meta_val_df.csv', index=False)\n",
    "meta_val_df = pd.read_csv('meta_val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4c1e4f-9de4-4f3c-b33e-91d99af43e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logi': {'accuracy': 0.79,\n",
       "  'precision': 0.7783132530120482,\n",
       "  'recall': 0.8095238095238095,\n",
       "  'f1_score': 0.7936117936117937},\n",
       " 'Deci': {'accuracy': 0.81375,\n",
       "  'precision': 0.8272251308900523,\n",
       "  'recall': 0.7919799498746867,\n",
       "  'f1_score': 0.8092189500640204},\n",
       " 'KNei': {'accuracy': 0.89,\n",
       "  'precision': 0.8897243107769424,\n",
       "  'recall': 0.8897243107769424,\n",
       "  'f1_score': 0.8897243107769424},\n",
       " 'Rand': {'accuracy': 0.905,\n",
       "  'precision': 0.8968058968058968,\n",
       "  'recall': 0.9147869674185464,\n",
       "  'f1_score': 0.9057071960297767},\n",
       " 'Extr': {'accuracy': 0.90875,\n",
       "  'precision': 0.9034653465346535,\n",
       "  'recall': 0.9147869674185464,\n",
       "  'f1_score': 0.9090909090909092},\n",
       " 'Supp': {'accuracy': 0.875,\n",
       "  'precision': 0.8804071246819338,\n",
       "  'recall': 0.8671679197994987,\n",
       "  'f1_score': 0.8737373737373737}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate model metrics\n",
    "accuracy_dict = calculate_model_metrics(meta_val_df, f'{target}')\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d55e3d4-4ab3-49d0-a93a-5826b7e210d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_base</th>\n",
       "      <th>precision_base</th>\n",
       "      <th>recall_base</th>\n",
       "      <th>f1_score_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logi</td>\n",
       "      <td>0.79000</td>\n",
       "      <td>0.778313</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.793612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deci</td>\n",
       "      <td>0.81375</td>\n",
       "      <td>0.827225</td>\n",
       "      <td>0.791980</td>\n",
       "      <td>0.809219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNei</td>\n",
       "      <td>0.89000</td>\n",
       "      <td>0.889724</td>\n",
       "      <td>0.889724</td>\n",
       "      <td>0.889724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rand</td>\n",
       "      <td>0.90500</td>\n",
       "      <td>0.896806</td>\n",
       "      <td>0.914787</td>\n",
       "      <td>0.905707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extr</td>\n",
       "      <td>0.90875</td>\n",
       "      <td>0.903465</td>\n",
       "      <td>0.914787</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Supp</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.880407</td>\n",
       "      <td>0.867168</td>\n",
       "      <td>0.873737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy_base  precision_base  recall_base  f1_score_base\n",
       "0  Logi        0.79000        0.778313     0.809524       0.793612\n",
       "1  Deci        0.81375        0.827225     0.791980       0.809219\n",
       "2  KNei        0.89000        0.889724     0.889724       0.889724\n",
       "3  Rand        0.90500        0.896806     0.914787       0.905707\n",
       "4  Extr        0.90875        0.903465     0.914787       0.909091\n",
       "5  Supp        0.87500        0.880407     0.867168       0.873737"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "base_model_accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index').reset_index()\n",
    "base_model_accuracy_df.columns = ['Model', 'Accuracy_base','precision_base','recall_base','f1_score_base',]\n",
    "base_model_accuracy_df.to_csv('base_model_accuracy.csv', index=False)\n",
    "base_model_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720589af-e238-4194-9ebe-d415a6edba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
