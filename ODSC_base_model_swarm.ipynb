{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83e5892-afde-4eaa-8d59-424ea6762ba9",
   "metadata": {},
   "source": [
    "## Base Model Swarm\n",
    "This is the second in a series of three notebooks for the ODSC presentation 'Harnessing GPT Assistants for Superior Model Ensembles: A Beginner's Guide to AI STacked-Classifiers' ODSC East -- Jason Merwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec730270-3fb1-4037-94be-d130ee73b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import io\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# define functions\n",
    "\n",
    "def delete_all_agents():\n",
    "    ''' Deletes all exising Assistants '''\n",
    "    # Fetch the list of assistants\n",
    "    my_assistants = client.beta.assistants.list(order=\"desc\", limit=20)\n",
    "    asst_ids = [asst.id for asst in my_assistants.data]\n",
    "    print(f'Deleting {len(asst_ids)} assistants.')\n",
    "    # Delete each assistant\n",
    "    for asst_id in asst_ids:\n",
    "        client.beta.assistants.delete(asst_id)\n",
    "        print(f\"Deleted assistant with ID: {asst_id}\")\n",
    "    print('Finished deleting all assistants')\n",
    "    \n",
    "def delete_all_assistant_files():\n",
    "    ''' Deletes all exising files uploaded to client using API key '''\n",
    "    # generate a files object\n",
    "    files_object = client.files.list()\n",
    "    # get a list comprehension\n",
    "    file_ids = [file.id for file in files_object.data]\n",
    "    print(f'Deleting {len(file_ids)} files.')\n",
    "    #delete them all\n",
    "    for file_id in file_ids:\n",
    "        client.files.delete(file_id)\n",
    "        print(f\"Deleted file with ID: {file_id}\")\n",
    "        time.sleep(1)\n",
    "    print('Finished deleting all files')   \n",
    "\n",
    "def upload_csv(file_name):\n",
    "    response = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"assistants\")\n",
    "    print(response)\n",
    "    file_id = response.id\n",
    "    return file_id\n",
    "\n",
    "def spin_up(target, base_instructions, file_id):\n",
    "    # create assistant\n",
    "    my_assistant = client.beta.assistants.create(\n",
    "        instructions=base_instructions,\n",
    "        name=\"agent\",\n",
    "        tools=[{\"type\": \"code_interpreter\"}],\n",
    "        model=\"gpt-4-turbo-preview\", #\"gpt-4-1106-preview\", # \"gpt-4\", # \"gpt-3.5-turbo-1106\", \"gpt-4-turbo-preview\"\n",
    "        file_ids=file_id)\n",
    "    message_string = \"Please execute your ACTIONS on the csv file, the target field is \" + target\n",
    "    # Create a Thread\n",
    "    thread = client.beta.threads.create()\n",
    "    # Add a Message to a Thread\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= message_string)\n",
    "    # Run the Assistant\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=my_assistant.id)\n",
    "    return my_assistant, thread, run \n",
    "    print('Finished creating Assistants')\n",
    "    \n",
    "def catch_response(assistant, thread, run):\n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id)\n",
    "    print('########################')\n",
    "    print('Checking for response...')\n",
    "    # Handle None response\n",
    "    if run_status is None:\n",
    "        print(\"No response yet\")\n",
    "        return None, None  # Return a tuple of None values to match the expected return type\n",
    "    # Handle non-completed response\n",
    "    if run_status.status != 'completed':\n",
    "        print(\"Response status is not 'completed'\")\n",
    "        return None, None\n",
    "    # Handle completed response\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id)\n",
    "        contents = []  # Initialize an empty list to store contents\n",
    "        # Loop through messages and process content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "                contents.append(content)  # Append content to the list\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        return messages, contents  # Return messages and a list of contents\n",
    "    else:\n",
    "        print('Unable to retrieve message')\n",
    "        return None, None\n",
    "\n",
    "def create_dataframes_from_messages(messages, client):\n",
    "    loop_dfs = []\n",
    "    # Accessing the first ThreadMessage\n",
    "    first_thread_message = messages.data[0]  \n",
    "    message_ids = first_thread_message.file_ids\n",
    "    # Loop through each file ID and create a DataFrame\n",
    "    for file_id in message_ids:\n",
    "        # Read the file content\n",
    "        file_data = client.files.content(file_id)\n",
    "        file_data_bytes = file_data.read()\n",
    "        file_like_object = io.BytesIO(file_data_bytes)\n",
    "        # Create a DataFrame from the file-like object and append\n",
    "        df = pd.read_csv(file_like_object)\n",
    "        loop_dfs.append(df)\n",
    "    return loop_dfs    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef63290-48df-46eb-bff6-1477c980f267",
   "metadata": {},
   "source": [
    "# Initialize API Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d31fc1-742a-4eaa-b2a8-bb4206640610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659c00-3167-48f1-9a94-c5d7f22ea536",
   "metadata": {},
   "source": [
    "# check training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065661d2-92bd-4bf4-94fc-ab868c390f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use the feature engineer output\n",
    "target = 'Quality'\n",
    "encoded_train = pd.read_csv('feature_engineer_output_1.csv')\n",
    "\n",
    "#optional: use the original dataset instead\n",
    "#encoded_train = pd.read_csv('pre_assistant_train.csv')\n",
    "\n",
    "#add a row id \n",
    "encoded_train = encoded_train.reset_index()\n",
    "encoded_train = encoded_train.rename(columns={'index': 'row_id'})\n",
    "encoded_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ef89a-6978-4105-82b5-18dcc82c5334",
   "metadata": {},
   "source": [
    "# Create the Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5a694-eb7f-481d-aab0-97ad411d0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make sure any existing bots and files are cleaned up\n",
    "delete_all_agents()   \n",
    "delete_all_assistant_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcda3ab-fdfb-406f-853d-42bb9a4fe467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reserve 20% of training data to be used as \"inference\" data\n",
    "train_set, val_set = train_test_split(encoded_train, test_size=0.2)\n",
    "\n",
    "#save the files\n",
    "train_set.to_csv('encoded_train.csv', index=False)\n",
    "val_set.to_csv('encoded_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c3a52-3e56-4fe9-9496-6a56368806b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the model types here by description\n",
    "model_types = ['Logistic_Regression', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'Random_Forest', 'Extra_Trees_Random_Forest', 'Support Vector Machine']\n",
    "\n",
    "train_id = upload_csv(f'encoded_train.csv')\n",
    "val_id = upload_csv(f'encoded_val.csv')\n",
    "file_ids = [train_id, val_id]\n",
    "\n",
    "agents = []\n",
    "\n",
    "for i in model_types:\n",
    "    print(f'Creating {i} assistant')\n",
    "    \n",
    "    #assign loop version of models and file names\n",
    "    model = i\n",
    "    print('provided these files')\n",
    "    print(file_ids)\n",
    "    instructions = instructions = f'''\n",
    "    You are a data scientist who will build and test a predictive model with data from the provided csv file.\n",
    "    This model will be base model for a stacked model ensemble, thus the predictions on the training data will be used as input for a meta model. \n",
    "    When the user asks you to perform your ACTIONS, carry out the described ACTIONS on the provided files.\n",
    "    The target variable is '{target}'.\n",
    "    There is an id column to be maintained, unaltered and returned in the output called \"row_id\". This column should be excluded when training the model.\n",
    "\n",
    "    ACTIONS:\n",
    "\n",
    "    1.The data has been prepared for training a {model} classification model to predict the target variable '{target}'.\n",
    "    2.Split the training data in the file {train_id} into 5 K folds for cross-validation. Each fold should serve once as a validation set while the remaining folds serve as training sets.\n",
    "    3.Train a {model} classification model using default hyper-parameter values on each training set derived from the K folds, ensuring the target variable is '{target}'.\n",
    "    4.For each fold, use the trained {model} to predict the '{target}' on its corresponding validation set. Ensure the predictions are probabilities.\n",
    "    5.Compile the out-of-fold predictions into a single dataset. This dataset should include the 'row_id' from the testing set and the predicted probabilities. Name the columns as follows: 'row_id' and '{model[:4]}_prob'.\n",
    "    6.Save this compiled dataset as a CSV file named 'out_of_fold_predictions.csv' and prepare it for the user to download. This file will be used for training the meta-model.\n",
    "    7.Now use the trained models to score the validation data in the file {val_id} containing the same target column '{target}'. Average their scores for each row in the validation data and compile the results in the same way as before and prepare it for the user to download as a CSV file names 'valiation_predicitons.csv'\n",
    "    8.Both tables should contain 2 columns: row_id and '{model[:4]}_prob'.\n",
    "    9.Please only respond once, with both tables once they are ready for download.\n",
    "    \n",
    "    DO NOT:\n",
    "    1. Do not return any images.\n",
    "    2. Do not return any other tables besides the tables 'out_of_fold_predictions.csv' and 'valiation_predicitons.csv'\n",
    "    3. Do not include row_id as a feature in the training of the model.\n",
    "    4. Do not respond before both tables are ready for download.\n",
    "\n",
    "    '''  \n",
    "\n",
    "    # spin up for each model type and store return object\n",
    "    assistant, thread, run = spin_up(f'{target}', instructions, file_ids) \n",
    "    agents.append((assistant, thread, run, model))  \n",
    "    print()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30fa14-0bda-4790-aebf-277b4b0e2816",
   "metadata": {},
   "source": [
    "# Catch the Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82616b43-e941-4ded-af4a-40628e6c16fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run a loop to catch the Agent responses\n",
    "time.sleep(360) \n",
    "\n",
    "agent_responses = []\n",
    "for assistant, thread, run, model, in agents:\n",
    "    messages, content = catch_response(assistant, thread, run) \n",
    "    agent_responses.append((messages, content, model, assistant))\n",
    "    time.sleep(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4facb0a0-342f-42f5-a566-2e1fc0872374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract dataframes and compile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframes_from_messages(messages, client):\n",
    "    loop_dfs = []\n",
    "\n",
    "    # Check if messages is None or messages.data is empty\n",
    "    if messages is None or not messages.data:\n",
    "        print(\"No messages data found.\")\n",
    "        return loop_dfs\n",
    "\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "\n",
    "    # Loop through each file ID and create a DataFrame\n",
    "    for file_id in message_ids:\n",
    "        # Read the file content\n",
    "        file_data = client.files.content(file_id)\n",
    "\n",
    "        # Check if file_data is None\n",
    "        if file_data is None:\n",
    "            print(f\"No content found for file_id: {file_id}\")\n",
    "            continue  # Skip this iteration and proceed with the next file_id\n",
    "\n",
    "        file_data_bytes = file_data.read()\n",
    "        file_like_object = io.BytesIO(file_data_bytes)\n",
    "\n",
    "        # Create a DataFrame from the file-like object and append\n",
    "        df = pd.read_csv(file_like_object)\n",
    "        loop_dfs.append(df)\n",
    "\n",
    "    return loop_dfs\n",
    "\n",
    "df_list = []\n",
    "for messages, content, model, assistant in agent_responses:\n",
    "    dataframes = create_dataframes_from_messages(messages, client)\n",
    "    assistant_id = assistant.id\n",
    "    df_list.append([dataframes, model, assistant_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c9e2b-47f0-42c7-b8ea-4f1be2e23e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Capture the validation data scores\n",
    "val_data_df_dict = {}\n",
    "val_failures = []\n",
    "\n",
    "# Loop through and capture validation data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][0]) \n",
    "        print(df1)\n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        val_data_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        val_failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(val_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45598af6-ca07-41e4-b843-826db1ff4336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Capture the meta training data\n",
    "test_data_df_dict = {}\n",
    "test_failures = []\n",
    "\n",
    "# Loop through and capture testing data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][1]) \n",
    "        print(df1)\n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        test_data_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        test_failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(test_failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadc690-9123-4849-9358-a9c4cbfa9e7d",
   "metadata": {},
   "source": [
    "# Prepare Scored Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697748d1-16ab-4292-9082-b5eea9efb87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_val_keys = list(val_data_df_dict.keys())\n",
    "first_val_key = list_of_val_keys[0]\n",
    "meta_val_data = val_data_df_dict[first_val_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in val_data_df_dict:\n",
    "    if key != first_val_key and key not in val_failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = val_data_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_val_data = meta_val_data.merge(cols_to_join, on='row_id', how='left')\n",
    "        print(f'joined to {key}')\n",
    "\n",
    "# add back label\n",
    "val_label_df = encoded_train[['row_id', f'{target}']]\n",
    "meta_val_data = meta_val_data.merge(val_label_df, on='row_id', how='left')\n",
    "\n",
    "display(meta_val_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc1c86-f649-4e4b-8e2b-a71c8e3db14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_keys = list(test_data_df_dict.keys())\n",
    "first_key = list_of_keys[0]\n",
    "meta_training_data = test_data_df_dict[first_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in test_data_df_dict:\n",
    "    if key != first_key and key not in test_failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = test_data_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_training_data = meta_training_data.merge(cols_to_join, on='row_id', how='left')\n",
    "        print(f'joined to {key}')\n",
    "\n",
    "# add back label\n",
    "label_df = train_set[['row_id', f'{target}']]\n",
    "meta_training_data = meta_training_data.merge(label_df, on='row_id', how='left')\n",
    "\n",
    "display(meta_training_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d2418-f5ff-49f6-bc10-89796c0ea817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the meta training file\n",
    "meta_training_data.to_csv('meta_train_df.csv', index=False)\n",
    "meta_train_df = pd.read_csv('meta_train_df.csv')\n",
    "\n",
    "# save the meta validation file (acting as inference data)\n",
    "meta_val_data.to_csv('meta_val_df.csv', index=False)\n",
    "meta_val_df = pd.read_csv('meta_val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4da68-e9f6-4fef-805d-5764eeca9032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_model_accuracies(df):\n",
    "    model_accuracy_dict = {}\n",
    "    # Filter columns that contain probability predictions\n",
    "    prediction_columns = [col for col in df.columns if \"_prob\" in col]\n",
    "    \n",
    "    for col in prediction_columns:\n",
    "        # Assuming binary classification with 0.5 threshold\n",
    "        predicted_classes = df[col].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        actual_classes = df[f'{target}']\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "        # Extract model name from column name \n",
    "        model_name = col.split(\"_prob\")[0]\n",
    "        model_accuracy_dict[model_name] = accuracy\n",
    "    \n",
    "    return model_accuracy_dict\n",
    "\n",
    "accuracy_dict = calculate_model_accuracies(meta_val_df)\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced4e17-0402-4012-8888-6fb52fa952bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "base_model_accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index').reset_index()\n",
    "base_model_accuracy_df.columns = ['Model', 'Accuracy_base']\n",
    "base_model_accuracy_df.to_csv('base_model_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc6ac4-d135-4240-a34d-a0cdcc2f6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595e730-86d6-445f-901e-094decebbf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
