{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83e5892-afde-4eaa-8d59-424ea6762ba9",
   "metadata": {},
   "source": [
    "## Base Model Swarm\n",
    "This is the second in a series of three notebooks for the ODSC presentation 'Harnessing GPT Assistants for Superior Model Ensembles: A Beginner's Guide to AI STacked-Classifiers' ODSC East -- Jason Merwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec730270-3fb1-4037-94be-d130ee73b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import io\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# define functions\n",
    "\n",
    "def delete_all_agents():\n",
    "    ''' Deletes all exising Assistants '''\n",
    "    # Fetch the list of assistants\n",
    "    my_assistants = client.beta.assistants.list(order=\"desc\", limit=20)\n",
    "    asst_ids = [asst.id for asst in my_assistants.data]\n",
    "    print(f'Deleting {len(asst_ids)} assistants.')\n",
    "    # Delete each assistant\n",
    "    for asst_id in asst_ids:\n",
    "        client.beta.assistants.delete(asst_id)\n",
    "        print(f\"Deleted assistant with ID: {asst_id}\")\n",
    "    print('Finished deleting all assistants')\n",
    "    \n",
    "def delete_all_assistant_files():\n",
    "    ''' Deletes all exising files uploaded to client using API key '''\n",
    "    # generate a files object\n",
    "    files_object = client.files.list()\n",
    "    # get a list comprehension\n",
    "    file_ids = [file.id for file in files_object.data]\n",
    "    print(f'Deleting {len(file_ids)} files.')\n",
    "    #delete them all\n",
    "    for file_id in file_ids:\n",
    "        client.files.delete(file_id)\n",
    "        print(f\"Deleted file with ID: {file_id}\")\n",
    "        time.sleep(1)\n",
    "    print('Finished deleting all files')   \n",
    "\n",
    "def upload_csv(file_name):\n",
    "    response = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"assistants\")\n",
    "    print(response)\n",
    "    file_id = response.id\n",
    "    return file_id\n",
    "\n",
    "def spin_up(target, base_instructions, file_id):\n",
    "    # create assistant\n",
    "    my_assistant = client.beta.assistants.create(\n",
    "        instructions=base_instructions,\n",
    "        name=\"agent\",\n",
    "        tools=[{\"type\": \"code_interpreter\"}],\n",
    "        model=\"gpt-4-turbo-preview\", #\"gpt-4-1106-preview\", # \"gpt-4\", # \"gpt-3.5-turbo-1106\", \"gpt-4-turbo-preview\"\n",
    "        file_ids=file_id)\n",
    "    message_string = \"Please execute your ACTIONS on the csv file, the target field is \" + target\n",
    "    # Create a Thread\n",
    "    thread = client.beta.threads.create()\n",
    "    # Add a Message to a Thread\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content= message_string)\n",
    "    # Run the Assistant\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=my_assistant.id)\n",
    "    return my_assistant, thread, run \n",
    "    print('Finished creating Assistants')\n",
    "    \n",
    "def catch_response(assistant, thread, run):\n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id)\n",
    "    print('########################')\n",
    "    print('Checking for response...')\n",
    "    # Handle None response\n",
    "    if run_status is None:\n",
    "        print(\"No response yet\")\n",
    "        return None, None  # Return a tuple of None values to match the expected return type\n",
    "    # Handle non-completed response\n",
    "    if run_status.status != 'completed':\n",
    "        print(\"Response status is not 'completed'\")\n",
    "        return None, None\n",
    "    # Handle completed response\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id)\n",
    "        contents = []  # Initialize an empty list to store contents\n",
    "        # Loop through messages and process content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "                contents.append(content)  # Append content to the list\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        return messages, contents  # Return messages and a list of contents\n",
    "    else:\n",
    "        print('Unable to retrieve message')\n",
    "        return None, None\n",
    "\n",
    "def create_dataframes_from_messages(messages, client):\n",
    "    loop_dfs = []\n",
    "    # Accessing the first ThreadMessage\n",
    "    first_thread_message = messages.data[0]  \n",
    "    message_ids = first_thread_message.file_ids\n",
    "    # Loop through each file ID and create a DataFrame\n",
    "    for file_id in message_ids:\n",
    "        # Read the file content\n",
    "        file_data = client.files.content(file_id)\n",
    "        file_data_bytes = file_data.read()\n",
    "        file_like_object = io.BytesIO(file_data_bytes)\n",
    "        # Create a DataFrame from the file-like object and append\n",
    "        df = pd.read_csv(file_like_object)\n",
    "        loop_dfs.append(df)\n",
    "    return loop_dfs    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef63290-48df-46eb-bff6-1477c980f267",
   "metadata": {},
   "source": [
    "# Initialize API Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d31fc1-742a-4eaa-b2a8-bb4206640610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00659c00-3167-48f1-9a94-c5d7f22ea536",
   "metadata": {},
   "source": [
    "# check training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "065661d2-92bd-4bf4-94fc-ab868c390f90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>A_id</th>\n",
       "      <th>Size</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Sweetness</th>\n",
       "      <th>Crunchiness</th>\n",
       "      <th>Juiciness</th>\n",
       "      <th>Ripeness</th>\n",
       "      <th>Acidity</th>\n",
       "      <th>Quality</th>\n",
       "      <th>Sweetness*Ripeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.970049</td>\n",
       "      <td>-2.512336</td>\n",
       "      <td>5.346330</td>\n",
       "      <td>-1.012009</td>\n",
       "      <td>1.844900</td>\n",
       "      <td>0.329840</td>\n",
       "      <td>-0.491590</td>\n",
       "      <td>1</td>\n",
       "      <td>1.763432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.195217</td>\n",
       "      <td>-2.839257</td>\n",
       "      <td>3.664059</td>\n",
       "      <td>1.588232</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.867530</td>\n",
       "      <td>-0.722809</td>\n",
       "      <td>1</td>\n",
       "      <td>3.178681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.292024</td>\n",
       "      <td>-1.351282</td>\n",
       "      <td>-1.738429</td>\n",
       "      <td>-0.342616</td>\n",
       "      <td>2.838636</td>\n",
       "      <td>-0.038033</td>\n",
       "      <td>2.621636</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.657196</td>\n",
       "      <td>-2.271627</td>\n",
       "      <td>1.324874</td>\n",
       "      <td>-0.097875</td>\n",
       "      <td>3.637970</td>\n",
       "      <td>-3.413761</td>\n",
       "      <td>0.790723</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.522803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.364217</td>\n",
       "      <td>-1.296612</td>\n",
       "      <td>-0.384658</td>\n",
       "      <td>-0.553006</td>\n",
       "      <td>3.030874</td>\n",
       "      <td>-1.303849</td>\n",
       "      <td>0.501984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>3995</td>\n",
       "      <td>3995.0</td>\n",
       "      <td>0.059386</td>\n",
       "      <td>-1.067408</td>\n",
       "      <td>-3.714549</td>\n",
       "      <td>0.473052</td>\n",
       "      <td>1.697986</td>\n",
       "      <td>2.244055</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.335650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>3996</td>\n",
       "      <td>3996.0</td>\n",
       "      <td>-0.293118</td>\n",
       "      <td>1.949253</td>\n",
       "      <td>-0.204020</td>\n",
       "      <td>-0.640196</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>-1.087900</td>\n",
       "      <td>1.854235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.221953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3997</td>\n",
       "      <td>3997.0</td>\n",
       "      <td>-2.634515</td>\n",
       "      <td>-2.138247</td>\n",
       "      <td>-2.440461</td>\n",
       "      <td>0.657223</td>\n",
       "      <td>2.199709</td>\n",
       "      <td>4.763859</td>\n",
       "      <td>-1.334611</td>\n",
       "      <td>0</td>\n",
       "      <td>-11.626014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>3998</td>\n",
       "      <td>3998.0</td>\n",
       "      <td>-4.008004</td>\n",
       "      <td>-1.779337</td>\n",
       "      <td>2.366397</td>\n",
       "      <td>-0.200329</td>\n",
       "      <td>2.161435</td>\n",
       "      <td>0.214488</td>\n",
       "      <td>-2.229720</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>3999</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>0.278540</td>\n",
       "      <td>-1.715505</td>\n",
       "      <td>0.121217</td>\n",
       "      <td>-1.154075</td>\n",
       "      <td>1.266677</td>\n",
       "      <td>-0.776571</td>\n",
       "      <td>1.599796</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.094134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id    A_id      Size    Weight  Sweetness  Crunchiness  Juiciness  \\\n",
       "0          0     0.0 -3.970049 -2.512336   5.346330    -1.012009   1.844900   \n",
       "1          1     1.0 -1.195217 -2.839257   3.664059     1.588232   0.853286   \n",
       "2          2     2.0 -0.292024 -1.351282  -1.738429    -0.342616   2.838636   \n",
       "3          3     3.0 -0.657196 -2.271627   1.324874    -0.097875   3.637970   \n",
       "4          4     4.0  1.364217 -1.296612  -0.384658    -0.553006   3.030874   \n",
       "...      ...     ...       ...       ...        ...          ...        ...   \n",
       "3995    3995  3995.0  0.059386 -1.067408  -3.714549     0.473052   1.697986   \n",
       "3996    3996  3996.0 -0.293118  1.949253  -0.204020    -0.640196   0.024523   \n",
       "3997    3997  3997.0 -2.634515 -2.138247  -2.440461     0.657223   2.199709   \n",
       "3998    3998  3998.0 -4.008004 -1.779337   2.366397    -0.200329   2.161435   \n",
       "3999    3999  3999.0  0.278540 -1.715505   0.121217    -1.154075   1.266677   \n",
       "\n",
       "      Ripeness   Acidity  Quality  Sweetness*Ripeness  \n",
       "0     0.329840 -0.491590        1            1.763432  \n",
       "1     0.867530 -0.722809        1            3.178681  \n",
       "2    -0.038033  2.621636        0            0.066118  \n",
       "3    -3.413761  0.790723        1           -4.522803  \n",
       "4    -1.303849  0.501984        1            0.501536  \n",
       "...        ...       ...      ...                 ...  \n",
       "3995  2.244055  0.137784        0           -8.335650  \n",
       "3996 -1.087900  1.854235        1            0.221953  \n",
       "3997  4.763859 -1.334611        0          -11.626014  \n",
       "3998  0.214488 -2.229720        1            0.507565  \n",
       "3999 -0.776571  1.599796        1           -0.094134  \n",
       "\n",
       "[4000 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the feature engineer output\n",
    "target = 'Quality'\n",
    "encoded_train = pd.read_csv('feature_engineer_output_1.csv')\n",
    "\n",
    "#optional: use the original dataset instead\n",
    "#encoded_train = pd.read_csv('pre_assistant_train.csv')\n",
    "\n",
    "#add a row id \n",
    "encoded_train = encoded_train.reset_index()\n",
    "encoded_train = encoded_train.rename(columns={'index': 'row_id'})\n",
    "encoded_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ef89a-6978-4105-82b5-18dcc82c5334",
   "metadata": {},
   "source": [
    "# Create the Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e5a694-eb7f-481d-aab0-97ad411d0727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting 1 assistants.\n",
      "Deleted assistant with ID: asst_zYXRvRdWoOHxpM7ccSZxXAzs\n",
      "Finished deleting all assistants\n",
      "Deleting 3 files.\n",
      "Deleted file with ID: file-WPl1MRAWeI7IWrB2dX8qSHBn\n",
      "Deleted file with ID: file-GAbMdm5VPjrTG99b5F4PgOGh\n",
      "Deleted file with ID: file-7bc0x9cQvAn8ZanwDMCRV2rP\n",
      "Finished deleting all files\n"
     ]
    }
   ],
   "source": [
    "# first make sure any existing bots and files are cleaned up\n",
    "delete_all_agents()   \n",
    "delete_all_assistant_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcda3ab-fdfb-406f-853d-42bb9a4fe467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reserve 20% of training data to be used as \"inference\" data\n",
    "train_set, val_set = train_test_split(encoded_train, test_size=0.2)\n",
    "\n",
    "#save the files\n",
    "train_set.to_csv('encoded_train.csv', index=False)\n",
    "val_set.to_csv('encoded_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463c3a52-3e56-4fe9-9496-6a56368806b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-sGpQPaxkIAc4iWohawjivY9h', bytes=384961, created_at=1713641335, filename='encoded_train.csv', object='file', purpose='assistants', status='processed', status_details=None)\n",
      "FileObject(id='file-D8vdpQlLAWNXNpR0TJalZowu', bytes=96214, created_at=1713641336, filename='encoded_val.csv', object='file', purpose='assistants', status='processed', status_details=None)\n",
      "Creating Logistic_Regression assistant\n",
      "provided these files\n",
      "['file-sGpQPaxkIAc4iWohawjivY9h', 'file-D8vdpQlLAWNXNpR0TJalZowu']\n",
      "\n",
      "Creating DecisionTreeClassifier assistant\n",
      "provided these files\n",
      "['file-sGpQPaxkIAc4iWohawjivY9h', 'file-D8vdpQlLAWNXNpR0TJalZowu']\n",
      "\n",
      "Creating KNeighborsClassifier assistant\n",
      "provided these files\n",
      "['file-sGpQPaxkIAc4iWohawjivY9h', 'file-D8vdpQlLAWNXNpR0TJalZowu']\n",
      "\n",
      "Creating Random_Forest assistant\n",
      "provided these files\n",
      "['file-sGpQPaxkIAc4iWohawjivY9h', 'file-D8vdpQlLAWNXNpR0TJalZowu']\n",
      "\n",
      "Creating Extra_Trees_Random_Forest assistant\n",
      "provided these files\n",
      "['file-sGpQPaxkIAc4iWohawjivY9h', 'file-D8vdpQlLAWNXNpR0TJalZowu']\n",
      "\n",
      "Creating Support Vector Machine assistant\n",
      "provided these files\n",
      "['file-sGpQPaxkIAc4iWohawjivY9h', 'file-D8vdpQlLAWNXNpR0TJalZowu']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define the model types here by description\n",
    "model_types = ['Logistic_Regression', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'Random_Forest', 'Extra_Trees_Random_Forest', 'Support Vector Machine']\n",
    "\n",
    "train_id = upload_csv(f'encoded_train.csv')\n",
    "val_id = upload_csv(f'encoded_val.csv')\n",
    "file_ids = [train_id, val_id]\n",
    "\n",
    "agents = []\n",
    "\n",
    "for i in model_types:\n",
    "    print(f'Creating {i} assistant')\n",
    "    \n",
    "    #assign loop version of models and file names\n",
    "    model = i\n",
    "    print('provided these files')\n",
    "    print(file_ids)\n",
    "    instructions = instructions = f'''\n",
    "    You are a data scientist who will build and test a predictive model with data from the provided csv file.\n",
    "    This model will be base model for a stacked model ensemble, thus the predictions on the training data will be used as input for a meta model. \n",
    "    When the user asks you to perform your ACTIONS, carry out the described ACTIONS on the provided files.\n",
    "    The target variable is '{target}'.\n",
    "    There is an id column to be maintained, unaltered and returned in the output called \"row_id\". This column should be excluded when training the model.\n",
    "\n",
    "    ACTIONS:\n",
    "\n",
    "    1.The data has been prepared for training a {model} classification model to predict the target variable '{target}'.\n",
    "    2.Split the training data in the file {train_id} into 5 K folds for cross-validation. Each fold should serve once as a validation set while the remaining folds serve as training sets.\n",
    "    3.Train a {model} classification model using default hyper-parameter values on each training set derived from the K folds, ensuring the target variable is '{target}'.\n",
    "    4.For each fold, use the trained {model} to predict the '{target}' on its corresponding validation set. Ensure the predictions are probabilities.\n",
    "    5.Compile the out-of-fold predictions into a single dataset. This dataset should include the 'row_id' from the testing set and the predicted probabilities. Name the columns as follows: 'row_id' and '{model[:4]}_prob'.\n",
    "    6.Save this compiled dataset as a CSV file named 'out_of_fold_predictions.csv' and prepare it for the user to download. This file will be used for training the meta-model.\n",
    "    7.Now use the trained models to score the validation data in the file {val_id} containing the same target column '{target}'. Average their scores for each row in the validation data and compile the results in the same way as before and prepare it for the user to download as a CSV file names 'valiation_predicitons.csv'\n",
    "    8.Both tables should contain 2 columns: row_id and '{model[:4]}_prob'.\n",
    "    9.Please only respond once, with both tables once they are ready for download.\n",
    "    \n",
    "    DO NOT:\n",
    "    1. Do not return any images.\n",
    "    2. Do not return any other tables besides the tables 'out_of_fold_predictions.csv' and 'valiation_predicitons.csv'\n",
    "    3. Do not include row_id as a feature in the training of the model.\n",
    "    4. Do not respond before both tables are ready for download.\n",
    "\n",
    "    '''  \n",
    "\n",
    "    # spin up for each model type and store return object\n",
    "    assistant, thread, run = spin_up(f'{target}', instructions, file_ids) \n",
    "    agents.append((assistant, thread, run, model))  \n",
    "    print()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30fa14-0bda-4790-aebf-277b4b0e2816",
   "metadata": {},
   "source": [
    "# Catch the Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82616b43-e941-4ded-af4a-40628e6c16fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "Checking for response...\n",
      "Assistant: I have executed the actions on the provided CSV file. Two files have been generated and are available for download:\n",
      "\n",
      "1. Out-of-fold predictions: [out_of_fold_predictions.csv](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "2. Validation predictions: [validation_predictions.csv](sandbox:/mnt/data/validation_predicitons.csv)\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: The actions have been completed, and the resulting files are ready for download:\n",
      "\n",
      "- Out-of-fold predictions: [Download out_of_fold_predictions.csv](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "\n",
      "- Validation predictions: [Download valiation_predicitons.csv](sandbox:/mnt/data/valiation_predicitons.csv)\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: The tables have been prepared as requested and are now available for you to download:\n",
      "\n",
      "- Out-of-fold predictions: [out_of_fold_predictions.csv](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- Validation predictions: [validation_predictions.csv](sandbox:/mnt/data/validation_predicitons.csv)\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: The tasks have been completed, and the datasets are now ready for download:\n",
      "\n",
      "- [Out-of-fold predictions](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- [Validation predictions](sandbox:/mnt/data/validation_predictions.csv)\n",
      "Assistant: I'll start by performing the following actions:\n",
      "\n",
      "1. Load the training data from the provided CSV file.\n",
      "2. Split the training data into 5 K-folds for cross-validation.\n",
      "3. Train a Random Forest classification model on each training set derived from the K-folds, using default hyper-parameter values.\n",
      "4. Use the trained model to predict 'Quality' on its corresponding validation set, ensuring predictions are probabilities.\n",
      "5. Compile the out-of-fold predictions into a single dataset including the 'row_id' and the predicted probabilities, saving it as 'out_of_fold_predictions.csv'.\n",
      "6. Use the trained models to score the validation data, average their scores for each row, compile the results, and save as 'validation_predictions.csv'.\n",
      "\n",
      "Let's commence with loading the data and proceeding with subsequent steps.\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: I have executed the ACTIONS as requested and prepared the two tables. \n",
      "\n",
      "- The `out_of_fold_predictions.csv` contains the out-of-fold predictions for the training data. You can download it using [this link](sandbox:/mnt/data/out_of_fold_predictions.csv).\n",
      "\n",
      "- The `validation_predictions.csv` contains the averaged scores for each row in the validation data. You can download it using [this link](sandbox:/mnt/data/valiation_predicitons.csv).\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n",
      "########################\n",
      "Checking for response...\n",
      "Assistant: The tasks have been completed, and the two requested CSV files are ready for download:\n",
      "\n",
      "- [Out of fold predictions CSV](sandbox:/mnt/data/out_of_fold_predictions.csv)\n",
      "- [Validation predictions CSV](sandbox:/mnt/data/valiation_predicitons.csv)\n",
      "User: Please execute your ACTIONS on the csv file, the target field is Quality\n"
     ]
    }
   ],
   "source": [
    "# run a loop to catch the Agent responses\n",
    "time.sleep(360) \n",
    "\n",
    "agent_responses = []\n",
    "for assistant, thread, run, model, in agents:\n",
    "    messages, content = catch_response(assistant, thread, run) \n",
    "    agent_responses.append((messages, content, model, assistant))\n",
    "    time.sleep(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4facb0a0-342f-42f5-a566-2e1fc0872374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#extract dataframes and compile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataframes_from_messages(messages, client):\n",
    "    loop_dfs = []\n",
    "\n",
    "    # Check if messages is None or messages.data is empty\n",
    "    if messages is None or not messages.data:\n",
    "        print(\"No messages data found.\")\n",
    "        return loop_dfs\n",
    "\n",
    "    first_thread_message = messages.data[0]  # Accessing the first ThreadMessage\n",
    "    message_ids = first_thread_message.file_ids\n",
    "\n",
    "    # Loop through each file ID and create a DataFrame\n",
    "    for file_id in message_ids:\n",
    "        # Read the file content\n",
    "        file_data = client.files.content(file_id)\n",
    "\n",
    "        # Check if file_data is None\n",
    "        if file_data is None:\n",
    "            print(f\"No content found for file_id: {file_id}\")\n",
    "            continue  # Skip this iteration and proceed with the next file_id\n",
    "\n",
    "        file_data_bytes = file_data.read()\n",
    "        file_like_object = io.BytesIO(file_data_bytes)\n",
    "\n",
    "        # Create a DataFrame from the file-like object and append\n",
    "        df = pd.read_csv(file_like_object)\n",
    "        loop_dfs.append(df)\n",
    "\n",
    "    return loop_dfs\n",
    "\n",
    "df_list = []\n",
    "for messages, content, model, assistant in agent_responses:\n",
    "    dataframes = create_dataframes_from_messages(messages, client)\n",
    "    assistant_id = assistant.id\n",
    "    df_list.append([dataframes, model, assistant_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6c9e2b-47f0-42c7-b8ea-4f1be2e23e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     row_id  Logi_prob\n",
      "0      3481   0.497679\n",
      "1      3485   0.056286\n",
      "2      1678   0.449668\n",
      "3       988   0.844107\n",
      "4      1774   0.743276\n",
      "..      ...        ...\n",
      "795    2756   0.038116\n",
      "796    1433   0.263993\n",
      "797     492   0.919761\n",
      "798     462   0.588633\n",
      "799     737   0.652521\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Deci_prob\n",
      "0      3481        0.0\n",
      "1      3485        0.0\n",
      "2      1678        0.4\n",
      "3       988        1.0\n",
      "4      1774        1.0\n",
      "..      ...        ...\n",
      "795    2756        0.0\n",
      "796    1433        0.0\n",
      "797     492        1.0\n",
      "798     462        0.8\n",
      "799     737        0.6\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  KNei_prob\n",
      "0      3481        0.8\n",
      "1      3485        0.4\n",
      "2      1678        0.0\n",
      "3       988        0.6\n",
      "4      1774        0.4\n",
      "..      ...        ...\n",
      "795    2756        0.4\n",
      "796    1433        0.4\n",
      "797     492        0.8\n",
      "798     462        0.6\n",
      "799     737        0.6\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Rand_prob\n",
      "0      3481      0.360\n",
      "1      3485      0.044\n",
      "2      1678      0.432\n",
      "3       988      0.956\n",
      "4      1774      0.802\n",
      "..      ...        ...\n",
      "795    2756      0.014\n",
      "796    1433      0.128\n",
      "797     492      0.980\n",
      "798     462      0.892\n",
      "799     737      0.546\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Extr_prob\n",
      "0      3481       0.45\n",
      "1      3485       0.04\n",
      "2      1678       0.41\n",
      "3       988       0.96\n",
      "4      1774       0.69\n",
      "..      ...        ...\n",
      "795    2756       0.02\n",
      "796    1433       0.10\n",
      "797     492       0.98\n",
      "798     462       0.84\n",
      "799     737       0.52\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "     row_id  Supp_prob\n",
      "0      3481   0.263397\n",
      "1      3485   0.022915\n",
      "2      1678   0.108065\n",
      "3       988   0.999231\n",
      "4      1774   0.967124\n",
      "..      ...        ...\n",
      "795    2756   0.010409\n",
      "796    1433   0.005122\n",
      "797     492   0.995047\n",
      "798     462   0.921077\n",
      "799     737   0.365588\n",
      "\n",
      "[800 rows x 2 columns]\n",
      "assistants which failed to return a scored training data dataframe:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Capture the validation data scores\n",
    "val_data_df_dict = {}\n",
    "val_failures = []\n",
    "\n",
    "# Loop through and capture validation data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][0]) \n",
    "        print(df1)\n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        val_data_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        val_failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(val_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45598af6-ca07-41e4-b843-826db1ff4336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      row_id  Logi_prob\n",
      "0          0   0.733640\n",
      "1          2   0.514789\n",
      "2          3   0.736664\n",
      "3          5   0.011677\n",
      "4          6   0.954484\n",
      "...      ...        ...\n",
      "3195    3994   0.782036\n",
      "3196    3995   0.109293\n",
      "3197    3996   0.676410\n",
      "3198    3997   0.021024\n",
      "3199    3998   0.621501\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Deci_prob\n",
      "0       3767        1.0\n",
      "1       1877        1.0\n",
      "2        459        0.0\n",
      "3       1562        1.0\n",
      "4       1758        1.0\n",
      "...      ...        ...\n",
      "3195    3331        1.0\n",
      "3196    3749        0.0\n",
      "3197    1073        1.0\n",
      "3198     574        1.0\n",
      "3199    2984        0.0\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  KNei_prob\n",
      "0       3767        0.8\n",
      "1       1005        0.2\n",
      "2       3951        0.8\n",
      "3       3864        0.4\n",
      "4       3883        0.2\n",
      "...      ...        ...\n",
      "3195    2648        0.2\n",
      "3196    3499        0.4\n",
      "3197    2878        0.4\n",
      "3198    1644        0.8\n",
      "3199    2984        0.4\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Rand_prob\n",
      "0       3767       0.77\n",
      "1       1005       0.16\n",
      "2       3951       0.79\n",
      "3       3864       0.81\n",
      "4       3883       0.57\n",
      "...      ...        ...\n",
      "3195    2648       0.03\n",
      "3196    3499       0.28\n",
      "3197    2878       0.03\n",
      "3198    1644       0.11\n",
      "3199    2984       0.07\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Extr_prob\n",
      "0       3767       0.61\n",
      "1       1005       0.19\n",
      "2       3951       0.57\n",
      "3       3864       0.77\n",
      "4       3883       0.51\n",
      "...      ...        ...\n",
      "3195    2648       0.10\n",
      "3196    3499       0.32\n",
      "3197    2878       0.07\n",
      "3198    1644       0.07\n",
      "3199    2984       0.09\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "      row_id  Supp_prob\n",
      "0       3767   0.816325\n",
      "1       1005   0.696164\n",
      "2       3951   0.003299\n",
      "3       3864   0.988412\n",
      "4       3883   0.723425\n",
      "...      ...        ...\n",
      "3195    2648   0.803339\n",
      "3196    3499   0.033433\n",
      "3197    2878   0.744092\n",
      "3198    1644   0.985428\n",
      "3199    2984   0.001632\n",
      "\n",
      "[3200 rows x 2 columns]\n",
      "assistants which failed to return a scored training data dataframe:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Capture the meta training data\n",
    "test_data_df_dict = {}\n",
    "test_failures = []\n",
    "\n",
    "# Loop through and capture testing data output\n",
    "for item in df_list:\n",
    "    try:\n",
    "        df1 = pd.DataFrame(item[0][1]) \n",
    "        print(df1)\n",
    "        if 'row_id' not in df1.columns:\n",
    "            df1 = df1.reset_index().rename(columns={'index': 'row_id'})\n",
    "        model = item[1]\n",
    "        # Extract the first three letters of the model and the fold_id value\n",
    "        key = model\n",
    "        # Add the DataFrame to the dictionary with the generated key\n",
    "        test_data_df_dict[key] = df1\n",
    "        \n",
    "    except:\n",
    "        assistant_model = item[1]\n",
    "        test_failures.append([assistant_model])\n",
    "        \n",
    "# Display failed data returns\n",
    "print('assistants which failed to return a scored training data dataframe:')\n",
    "print(test_failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadc690-9123-4849-9358-a9c4cbfa9e7d",
   "metadata": {},
   "source": [
    "# Prepare Scored Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697748d1-16ab-4292-9082-b5eea9efb87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined to DecisionTreeClassifier\n",
      "joined to KNeighborsClassifier\n",
      "joined to Random_Forest\n",
      "joined to Extra_Trees_Random_Forest\n",
      "joined to Support Vector Machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Logi_prob</th>\n",
       "      <th>Deci_prob</th>\n",
       "      <th>KNei_prob</th>\n",
       "      <th>Rand_prob</th>\n",
       "      <th>Extr_prob</th>\n",
       "      <th>Supp_prob</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3481</td>\n",
       "      <td>0.497679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.263397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3485</td>\n",
       "      <td>0.056286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.022915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1678</td>\n",
       "      <td>0.449668</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.108065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>988</td>\n",
       "      <td>0.844107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.999231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1774</td>\n",
       "      <td>0.743276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.967124</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>2756</td>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1433</td>\n",
       "      <td>0.263993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>492</td>\n",
       "      <td>0.919761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.995047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>462</td>\n",
       "      <td>0.588633</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.921077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>737</td>\n",
       "      <td>0.652521</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.365588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  Logi_prob  Deci_prob  KNei_prob  Rand_prob  Extr_prob  Supp_prob  \\\n",
       "0      3481   0.497679        0.0        0.8      0.360       0.45   0.263397   \n",
       "1      3485   0.056286        0.0        0.4      0.044       0.04   0.022915   \n",
       "2      1678   0.449668        0.4        0.0      0.432       0.41   0.108065   \n",
       "3       988   0.844107        1.0        0.6      0.956       0.96   0.999231   \n",
       "4      1774   0.743276        1.0        0.4      0.802       0.69   0.967124   \n",
       "..      ...        ...        ...        ...        ...        ...        ...   \n",
       "795    2756   0.038116        0.0        0.4      0.014       0.02   0.010409   \n",
       "796    1433   0.263993        0.0        0.4      0.128       0.10   0.005122   \n",
       "797     492   0.919761        1.0        0.8      0.980       0.98   0.995047   \n",
       "798     462   0.588633        0.8        0.6      0.892       0.84   0.921077   \n",
       "799     737   0.652521        0.6        0.6      0.546       0.52   0.365588   \n",
       "\n",
       "     Quality  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          1  \n",
       "..       ...  \n",
       "795        0  \n",
       "796        0  \n",
       "797        1  \n",
       "798        1  \n",
       "799        0  \n",
       "\n",
       "[800 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_val_keys = list(val_data_df_dict.keys())\n",
    "first_val_key = list_of_val_keys[0]\n",
    "meta_val_data = val_data_df_dict[first_val_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in val_data_df_dict:\n",
    "    if key != first_val_key and key not in val_failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = val_data_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_val_data = meta_val_data.merge(cols_to_join, on='row_id', how='left')\n",
    "        print(f'joined to {key}')\n",
    "\n",
    "# add back label\n",
    "val_label_df = encoded_train[['row_id', f'{target}']]\n",
    "meta_val_data = meta_val_data.merge(val_label_df, on='row_id', how='left')\n",
    "\n",
    "display(meta_val_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acc1c86-f649-4e4b-8e2b-a71c8e3db14c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joined to DecisionTreeClassifier\n",
      "joined to KNeighborsClassifier\n",
      "joined to Random_Forest\n",
      "joined to Extra_Trees_Random_Forest\n",
      "joined to Support Vector Machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Logi_prob</th>\n",
       "      <th>Deci_prob</th>\n",
       "      <th>KNei_prob</th>\n",
       "      <th>Rand_prob</th>\n",
       "      <th>Extr_prob</th>\n",
       "      <th>Supp_prob</th>\n",
       "      <th>Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.733640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.404457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.514789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.933348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.736664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.686683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.011677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.979651</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.954484</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.963144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>3994</td>\n",
       "      <td>0.782036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.623157</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>3995</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.361807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>3996</td>\n",
       "      <td>0.676410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.528052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>3997</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.892646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>3998</td>\n",
       "      <td>0.621501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.444566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id  Logi_prob  Deci_prob  KNei_prob  Rand_prob  Extr_prob  \\\n",
       "0          0   0.733640        1.0        0.4       0.88       0.86   \n",
       "1          2   0.514789        0.0        0.6       0.24       0.29   \n",
       "2          3   0.736664        1.0        0.2       0.87       0.91   \n",
       "3          5   0.011677        0.0        0.2       0.03       0.06   \n",
       "4          6   0.954484        1.0        0.6       0.95       0.93   \n",
       "...      ...        ...        ...        ...        ...        ...   \n",
       "3195    3994   0.782036        1.0        0.6       0.81       0.81   \n",
       "3196    3995   0.109293        0.0        0.2       0.03       0.04   \n",
       "3197    3996   0.676410        0.0        0.4       0.63       0.77   \n",
       "3198    3997   0.021024        0.0        0.2       0.03       0.00   \n",
       "3199    3998   0.621501        1.0        0.6       0.72       0.71   \n",
       "\n",
       "      Supp_prob  Quality  \n",
       "0      0.404457        1  \n",
       "1      0.933348        0  \n",
       "2      0.686683        1  \n",
       "3      0.979651        0  \n",
       "4      0.963144        1  \n",
       "...         ...      ...  \n",
       "3195   0.623157        1  \n",
       "3196   0.361807        0  \n",
       "3197   0.528052        1  \n",
       "3198   0.892646        0  \n",
       "3199   0.444566        1  \n",
       "\n",
       "[3200 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a target df to join everything to\n",
    "list_of_keys = list(test_data_df_dict.keys())\n",
    "first_key = list_of_keys[0]\n",
    "meta_training_data = test_data_df_dict[first_key]\n",
    "\n",
    "# Loop through the DataFrames in the dictionary, joining each to the label\n",
    "for key in test_data_df_dict:\n",
    "    if key != first_key and key not in test_failures:\n",
    "        # get each dataframe\n",
    "        cols_to_join = test_data_df_dict[key]\n",
    "        # Join with the initial DataFrame on 'row_id'\n",
    "        meta_training_data = meta_training_data.merge(cols_to_join, on='row_id', how='left')\n",
    "        print(f'joined to {key}')\n",
    "\n",
    "# add back label\n",
    "label_df = train_set[['row_id', f'{target}']]\n",
    "meta_training_data = meta_training_data.merge(label_df, on='row_id', how='left')\n",
    "\n",
    "display(meta_training_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300d2418-f5ff-49f6-bc10-89796c0ea817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the meta training file\n",
    "meta_training_data.to_csv('meta_train_df.csv', index=False)\n",
    "meta_train_df = pd.read_csv('meta_train_df.csv')\n",
    "\n",
    "# save the meta validation file (acting as inference data)\n",
    "meta_val_data.to_csv('meta_val_df.csv', index=False)\n",
    "meta_val_df = pd.read_csv('meta_val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca4da68-e9f6-4fef-805d-5764eeca9032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logi': 0.7825,\n",
       " 'Deci': 0.8475,\n",
       " 'KNei': 0.58875,\n",
       " 'Rand': 0.89375,\n",
       " 'Extr': 0.90125,\n",
       " 'Supp': 0.87875}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_model_accuracies(df):\n",
    "    model_accuracy_dict = {}\n",
    "    # Filter columns that contain probability predictions\n",
    "    prediction_columns = [col for col in df.columns if \"_prob\" in col]\n",
    "    \n",
    "    for col in prediction_columns:\n",
    "        # Assuming binary classification with 0.5 threshold\n",
    "        predicted_classes = df[col].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "        actual_classes = df[f'{target}']\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(actual_classes, predicted_classes)\n",
    "        # Extract model name from column name \n",
    "        model_name = col.split(\"_prob\")[0]\n",
    "        model_accuracy_dict[model_name] = accuracy\n",
    "    \n",
    "    return model_accuracy_dict\n",
    "\n",
    "accuracy_dict = calculate_model_accuracies(meta_val_df)\n",
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ced4e17-0402-4012-8888-6fb52fa952bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "base_model_accuracy_df = pd.DataFrame.from_dict(accuracy_dict, orient='index').reset_index()\n",
    "base_model_accuracy_df.columns = ['Model', 'Accuracy_base']\n",
    "base_model_accuracy_df.to_csv('base_model_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3bc6ac4-d135-4240-a34d-a0cdcc2f6ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logi</td>\n",
       "      <td>0.78250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deci</td>\n",
       "      <td>0.84750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNei</td>\n",
       "      <td>0.58875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rand</td>\n",
       "      <td>0.89375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Extr</td>\n",
       "      <td>0.90125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Supp</td>\n",
       "      <td>0.87875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy_base\n",
       "0  Logi        0.78250\n",
       "1  Deci        0.84750\n",
       "2  KNei        0.58875\n",
       "3  Rand        0.89375\n",
       "4  Extr        0.90125\n",
       "5  Supp        0.87875"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595e730-86d6-445f-901e-094decebbf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
