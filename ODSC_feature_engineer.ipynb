{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacd7e9f-7214-47f4-89c6-77616166425e",
   "metadata": {},
   "source": [
    "## Feature Engineer\n",
    "This is the first in a series of three notebooks for the ODSC presentation 'Harnessing GPT Assistants for Superior Model Ensembles: A Beginner's Guide to AI STacked-Classifiers' ODSC East -- Jason Merwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722296da-fa0f-4fee-98d0-31b78dafd9d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import io\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from config import OPENAI_API_KEY\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "## Define Functions \n",
    "def delete_all_agents():\n",
    "    ''' Deletes all exising Assistants associated with API key '''\n",
    "    # Fetch the list of assistants\n",
    "    my_assistants = client.beta.assistants.list(order=\"desc\", limit=20)\n",
    "    asst_ids = [asst.id for asst in my_assistants.data]\n",
    "    print(f'Deleting {len(asst_ids)} assistants.')\n",
    "    # Delete each assistant\n",
    "    for asst_id in asst_ids:\n",
    "        client.beta.assistants.delete(asst_id)\n",
    "        print(f\"Deleted assistant with ID: {asst_id}\")\n",
    "        time.sleep(1)\n",
    "    print('Finished deleting all assistants')\n",
    "    \n",
    "def delete_all_assistant_files():\n",
    "    ''' Deletes all exising files uploaded to OpenAI client using API key '''\n",
    "    # generate a files object\n",
    "    files_object = client.files.list()\n",
    "    # get a list comprehension\n",
    "    file_ids = [file.id for file in files_object.data]\n",
    "    print(f'Deleting {len(file_ids)} files.')\n",
    "    #delete them all\n",
    "    for file_id in file_ids:\n",
    "        client.files.delete(file_id)\n",
    "        print(f\"Deleted file with ID: {file_id}\")\n",
    "        time.sleep(1)\n",
    "    print('Finished deleting all files')  \n",
    "\n",
    "def upload_csv(file_name):\n",
    "    \"\"\"\n",
    "    Sends a csv file to OpenAI and returns the file id\n",
    "    file_name: string for name and ext of saved file. Example: \"analyst_output.csv\"\n",
    "    return: file id\n",
    "    \"\"\"\n",
    "    response = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"assistants\")\n",
    "    print(response)\n",
    "    file_id = response.id\n",
    "    return file_id\n",
    "\n",
    "def read_and_save_file(first_file_id, file_name):  \n",
    "    \"\"\"\n",
    "    Reads the file contents from OpenAI file id and saves as csv.\n",
    "    first_file_id: OpenAI file id\n",
    "    file_name: string for name and ext of saved file. Example: \"analyst_output.csv\"\n",
    "    \"\"\"\n",
    "    # its binary, so read it and then make it a file like object\n",
    "    file_data = client.files.content(first_file_id)\n",
    "    file_data_bytes = file_data.read()\n",
    "    file_like_object = io.BytesIO(file_data_bytes)\n",
    "    #now read as csv to create df\n",
    "    returned_data = pd.read_csv(file_like_object)\n",
    "    returned_data.to_csv(file_name, index=False)\n",
    "    return returned_data\n",
    "\n",
    "def files_from_messages(messages, asst_name):\n",
    "    \"\"\"\n",
    "    Returns a csv data file from an OpenAI API message object.\n",
    "    messages: OpenAI API messages object\n",
    "    asst_name: string name of Assistant for use when saving file\n",
    "    \"\"\"\n",
    "    first_thread_message = messages.data[0]  \n",
    "    message_ids = first_thread_message.file_ids\n",
    "    print(message_ids)\n",
    "    # Loop through each file ID and save the file with a sequential name\n",
    "    for i, file_id in enumerate(message_ids):\n",
    "        file_name = f\"{asst_name}_output_{i+1}.csv\" \n",
    "        read_and_save_file(file_id, file_name)\n",
    "        print(f'saved {file_name}')\n",
    "        \n",
    "def get_string_features(df):\n",
    "    \"\"\"\n",
    "    Returns a list of column names in the DataFrame that are of string type.\n",
    "    param df: pandas DataFrame\n",
    "    return: List of column names that are strings\n",
    "    \"\"\"\n",
    "    # Select columns of object dtype (commonly used for strings)\n",
    "    string_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    return string_columns  \n",
    "\n",
    "def convert_strings_to_numbers(df, columns_to_convert):\n",
    "    \"\"\"\n",
    "    Converts unique string values in specified columns to increasing numeric values.\n",
    "\n",
    "    param df: pandas DataFrame\n",
    "    param columns_to_convert: List of column names to be converted\n",
    "    return: DataFrame with specified columns converted to numeric values\n",
    "    \"\"\"\n",
    "    for column in columns_to_convert:\n",
    "        # Ensure the column is in the DataFrame\n",
    "        if column in df.columns:\n",
    "            # Create a mapping from unique strings to numbers\n",
    "            unique_strings = df[column].unique()\n",
    "            string_to_number_mapping = {string: i for i, string in enumerate(unique_strings)}\n",
    "            # Apply the mapping to the column\n",
    "            df[column] = df[column].map(string_to_number_mapping)\n",
    "        else:\n",
    "            print(f\"Column '{column}' not found in DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8356d113-b733-42e2-83dd-b9c4a13f342c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Upload Data Set and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9e2d7-1db7-44cf-b777-1d75d209265c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload data set and define target\n",
    "training_df = pd.read_csv('apple_quality.csv')\n",
    "target = 'Quality'\n",
    "print(training_df.shape)\n",
    "training_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10ea6a-579e-4daf-96e6-f4ecd30db9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of null values for each column\n",
    "percent_missing = training_df.isnull().mean() * 100\n",
    "print(percent_missing)\n",
    "\n",
    "# check class distribution\n",
    "normalized_distribution = training_df[f'{target}'].value_counts(normalize=True)\n",
    "print(f'The percentage of class values in the data set are {normalized_distribution}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009fc60-fc7e-4114-8d97-7482de581e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make copy of training data\n",
    "train_df = training_df.copy()\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978eea4-edbd-4ff3-9769-fa9ce9172b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the encoded training data\n",
    "train_df.to_csv('pre_assistant_train.csv', index=False)\n",
    "encoded_train = pd.read_csv('pre_assistant_train.csv')\n",
    "encoded_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b7d55-ebbd-434e-98a7-2dd0f45fe39b",
   "metadata": {},
   "source": [
    "# Send data set to Assistant for Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d88803-7838-4e19-90b5-0cca9fb58223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41487066-3fe3-4eb8-a2de-6f88215034e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up any existing assistants and files\n",
    "delete_all_agents()   \n",
    "delete_all_assistant_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de9041-87f3-4450-8fed-64415f926369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the assistant and give it the CSV file\n",
    "\n",
    "prompt = f'''\n",
    "You are a feature engineer who will create and test new features from a csv data set in your files. \n",
    "When the user asks you to perform your actions, use the csv file to read the data into a pandas dataframe.\n",
    "The data set contains predictive features that will be used for a binary classifier.\n",
    "Follow each of the steps listed below in your ACTIONS. The target variable is {target}. \n",
    "\n",
    "ACTIONS:\n",
    "\n",
    "1. Read the file data into a pandas DataFrame. \n",
    "2. Check for missing values and impute the column mean for those missing values.\n",
    "3. Numerically encode any categorical columns.\n",
    "4. Create new feature interaction columns using the continuous, non categorical columns. For each unique pair of columns create a new column that is the result of multiplying one column by the other and a second column that is the result of dividing one column by the other. \n",
    "5. Add these new features to the original data set and run an extra trees random forest with 3,000 trees to predict the target variable '{target}'. \n",
    "6. Get the feature importances of all the features in the model and prepare the feature importance values as Table_1. This table should have one column for the features name and one for the importance value.\n",
    "7. Now prepare a final training data set that contains the original continuous features, numerically encoded features, and the top 3 feature interactions based on their feature importance values. Prepare this final table as Table_2. Table 2 should have the 11 feature columns, the top 3 feature interactions, and the target {target}.\n",
    "8. Prepare Table_1 and Table_2 for download by the user. \n",
    "\n",
    "DO NOT:\n",
    "1. Do not return any images. \n",
    "2. Do not return any other tables besides Table_1 and Table_2\n",
    "3. Do not use the target column {target} in the feature interactions.\n",
    "4. Do not remove any of the original data set columns from the final data set Table_2.\n",
    "\n",
    "'''\n",
    "\n",
    "# send the csv file to the assistant purpose files\n",
    "training_file_id = upload_csv('pre_assistant_train.csv')\n",
    "\n",
    "# create the assistant and link to file\n",
    "my_assistant = client.beta.assistants.create(\n",
    "    instructions=prompt,\n",
    "    name=\"feature_engineer\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-turbo-preview\", \n",
    "    file_ids=[training_file_id]\n",
    ")\n",
    "\n",
    "# get the assistant file id\n",
    "fileId = my_assistant.file_ids[0]\n",
    "print(my_assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6971dae-a541-4057-bae8-d75266e42642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the request to the assistant\n",
    "message_string = f\"Please execute your ACTIONS on the data stored in the csv file {fileId}. The Target variable is {target}\"\n",
    "print(message_string)\n",
    "\n",
    "# Create a Thread\n",
    "thread = client.beta.threads.create()\n",
    "print('created thread')\n",
    "\n",
    "# Add a Message to a Thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content= message_string\n",
    ")\n",
    "print('added message to thread')\n",
    "\n",
    "# Run the Assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=my_assistant.id\n",
    "    #instructions=\"you can overwrite prompt instructions here\"\n",
    ")\n",
    "print('running the client')\n",
    "\n",
    "print('getting json response')\n",
    "print(run.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885aa600-f724-48c1-8ff2-2d759850ddbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let an initial 5 minutes pass\n",
    "time.sleep(360) \n",
    "\n",
    "# check for a response\n",
    "while True:\n",
    "    # Wait for 5 seconds\n",
    "    time.sleep(60)  \n",
    "    # Retrieve the run status\n",
    "    run_status = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print('One eternity later...')\n",
    "    # If run is completed, get messages\n",
    "    if run_status.status == 'completed':\n",
    "        messages = client.beta.threads.messages.list(\n",
    "            thread_id=thread.id\n",
    "        )\n",
    "        # Loop through messages and print content based on role\n",
    "        for msg in messages.data:\n",
    "            role = msg.role\n",
    "            try:\n",
    "                content = msg.content[0].text.value\n",
    "                print(f\"{role.capitalize()}: {content}\")\n",
    "            except AttributeError:\n",
    "                # This will execute if .text does not exist\n",
    "                print(f\"{role.capitalize()}: [Non-text content, possibly an image or other file type]\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6a529-8b19-477f-b643-76fafd6f4766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the file names from the response and retrieve the content\n",
    "asst_name = 'feature_engineer'        \n",
    "files_from_messages(messages, asst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba16c3e-fae2-44d9-902d-7b16f976f6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('feature_engineer_output_1.csv')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8a9c75-50ef-4425-82ad-f95578cbc8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df2 = pd.read_csv('feature_engineer_output_2.csv')\n",
    "df2 = df2.sort_values(by='Importance', ascending=False)\n",
    "display(df2.head())\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 25))\n",
    "plt.barh(df2['Feature'], df2['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)  \n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b9d89-0cb1-45b1-915e-0835b20cbf47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
